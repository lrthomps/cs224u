% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{CS224u lit review}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Lara Thompson \\
  Principle Data Scientist @ Salesforce \\
  \texttt{lara.thompson@salesforce.com} \\ \\
  \today \\
  % Second Author \\
  % Affiliation / Address  \\  
  % \texttt{email@domain} \\\And
  % Third Author \\
  % Affiliation / Address \\  
  % \texttt{email@domain}
}

\begin{document}

\maketitle

% \section*{Notes}

% This is a short paper ($\approx$6 page) summarizing and synthesizing several papers in the area of your final project. As noted above, 8 pages is the maximum allowed length.

% Groups of one should review 5 papers, groups of two should review 7 papers, and groups of three should review 9.

% The ideal is to have the same topic for your lit review and final project, but it's possible that you'll discover in the lit review that your topic isn't ideal for you, so you can switch topics (or groups) for the final project; your lit review will be graded on its own terms. 

% The following section headings are not required, but they nicely cover the main things you need to include.

% You are required to have a references section. The entries should appear alphabetically and give at least full author name(s), year of publication, title, and outlet if applicable (e.g., journal name or proceedings name). Beyond that, we are not picky about the format. Electronic references are fine but need to include the above information in addition to the link.

\section{General problem/task definition}

For my final project, I'm interested in the intersection of language models and recommender systems. As alluded to in the course, recommender systems are similar retrieval systems albeit with a different aim: rather than searching for a few key results to answer a query, a recommender system has no query and should be less reliant on a current context (even if some information is given, eg. last book read, we still don't know what the reader is in the mood for next). Regardless, there is much to learn from information retriever systems such as ColBERT \cite{ColBERT}. \cite{Malkiel2020} take a BERT-based approach to pure text content recommendations.  Hybrid recommender systems that leverate user-item and user-user graphs mostly use only very simple text encodings.  

Sentence-BERT \cite{Reimers2019} is a good paper to compare with ColBERT in their usage of pre-trained BERT and arguably is a better starting place for ColBERT's finetuning that BERT itself. "A simple but Tough-to-Beat Baseline for Sentence Embeddings" \cite{Arora2017} is a very interesting word embeddings aggregation approach that may win if system performance is more important than the last few points of accuracy. 

More recently, researchers from Alibaba propose M6-Rec...

\section{Concise summaries of the articles} 

\subsection{A Simple but Tough-to-Beat Baseline for Sentence Embeddings \cite{Arora2017}}

In contrast to Sentence-BERT, Arora et. al. from Princeton developed a very simple sentence embedding. They take a weighted average of the word embeddings (downweighting common words) then subtract out the projection to the principal component (as estimated across several sentences; again to remove the common discourse component).

To motivate their approach, they modify a random walk model of corpus generation. Rather than generating only words near a slowly changing discourse vector $c_s(t)$, they allow for the large deviations to common words ('the', 'and', etc) in two ways: first, by separating a common discourse component $c_0$ and, second, by allowing all words a chance to appear proportionally to their typical frequency $p(w)$. Note that $c_s$, $c_0$ and $v_w$, the word embedding of $w$ all reside in the same embedding space. The original probability of observing a word $w$ is:

\begin{equation}
  \mathrm{Pr}[w(t)|c_s(t)] \propto \exp\left(\left< c_s(t), v_w \right> \right)
\end{equation}

After the two modifications for common words and common syntax:
\begin{equation}
  \mathrm{Pr}[w(t)|c_s(t)] = \alpha p(w) + (1-\alpha) {\exp\left(\left< \tilde c_s(t), v_w \right> \right) \over Z_{\tilde c_s}}
\end{equation}
where $\tilde c_s = \beta c_0 + (1-\beta)c_s$. The authors then go on to show how this relates to a $p(w)$-weighted embedding average that then has its component along the principle component $\sim c_0$.

They test their sentence embeddings across the STS benchmarks and compare against other sentence embedding approaches (not Sentence BERT since it came out later!). They find they beat many more complicated approaches in text similarity tasks but the RNN approaches outperform them on sentiment tasks. It seems a bag-of-words approach cannot capture the combined sentiment; in fact, this approach may specifically downweight various negation terms (eg. "not") because they are common.

\subsection{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks \cite{Reimers2019}}

Reimers and Gurevych set out to train better sentence embeddings that the various word embeddings-based approaches for text similarity tasks that nevertheless scaled better than BERT /RoBERTa which much run on every pair individually. 

They use pretrained BERT and RoBERTa in two main configurations: in a siamese network finetuned on SNLI \cite{Bowman2015}, Multi-Genre NLI \cite{Williams2018} and (separately) on STS Argument Facet Similarity \cite{Misra2016} and STS benchmarks \cite{Cer2017} in a triplet network using the Wikipedia sections distinction dataset \cite{Dor2018}. 

The SNLI and multi NLI datasets are familiar from the course; the AFS dataset similarly has labelled pairs with a 0-5 similarity rating but, as argumentative excerpts from dialogue, the notion of similarity involves to the reasoning of the excerpts. While SBERT/SRoBERTa-NLI typically outperforms BERT on STS benchmarks, on the AFS dataset, the excerpt spanning attention in BERT appears to be important and SBERT-AFS lags by a several points in this task. 

The Wikipedia sections distinction dataset involves triplets: with two passages from one section of a Wikipedia article and a third from another section. They train with a triplet network (the three passages go through a single BERT model) and use a triplet loss that pushes the embeddings for similar passages closer and the dissimilar passages further in embedding space. This must be a challenging dataset to learn from: the sentences within a given section are not always semantically related; but it's an ingeneous way to generate a very large dataset. SBERT-WikiSec does quite well.

As a final evaluation, SBERT-NLI is tested in a transfer learning setting \cite{Conneau2018}: even though SBERT is intended to be finetuned to each task (here it is tested on held out tasks), it outperformed many other sentence embeddings. 

\subsection{ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT \cite{ColBERT}}

ColBERT sets out to leverage the full quality of BERT for contextualized information retrieval while reducing query time a hundred fold. Their design lands in the happy middle ground for query-document interactions in the network: late enough that the bulk of the expensive computations can be precomputed and indexed and yet there is an interaction layer that improves over completely separate query/document embeddings. 

Like SBERT, the same BERT model is used to encode both query and document ([Q]/[D] markers encodes which a sequence is). The query is padded with mask tokens that effectively augment the query in a differentiable way ("the [MASK] positions are represented by paying attention (in the 12th layer) to all other query tokens" \cite{okhat2020}). The output of the BERT model passes through a linear layer to reduce the final embedding dimenionality. The late interaction is the sum of maximum similarity between each query token and the document tokens. The BERT layers are fine-tuned while the linear layer and [Q]/[D] marker's embeddings are trained from scratch using a pairwise softmax cross-entropy loss on triples of $\left< q, d^+, d^-\right>$ (a query and positive + negative document match). 

The form of this late interaction allows for very efficient querying: from faster loading of embeddings into the GPU to a form amenable to optimized large-scale vector-similarity search (specifically, \texttt{faiss} \cite{faiss}).

ColBERT was evaluated on two information retrieval benchmark datasets: MS Marco Ranking \cite{MSMarco} and TREC Complex Retrieval \cite{TREC-CAR}. MRR@10 of ColBERT matched the far more costly direct adaptation of BERT to ranking \cite{Nogueira2019}: in "re-ranking" mode, it did a little better than BERT$_{base}$; in "end-to-end" mode, it did a little worse than BERT$_{large}$. In the ablation study, the biggest improvements in MRR@10 were for the maximum similarity in the late interaction and the query augmentation. 

% \subsection{Graph Neural Networks in Recommender Systems: A Survey \cite{GGN4R2022}}

% This survey paper from last year covers a wide breadth of design choices for GNNs used for recommenders. Their taxonomy of GNNs as recommenders include “user-item collaborative filtering, sequential recommendation, social recommendation, knowledge graph-based recommendation, and other tasks” [1]; they review the challenges and solutions for each category. They conclude with a few interesting avenues of future work. My main interests lie in a combination of the first two so I’ll focus on the main design choices in these applications.

% The two main challenges in user-item are effectiveness (how well can graph structure learn complex user-item relationships) and efficiency (how feasible is training the graph on the full neighbourhoods of each node). Any solution is a compromise between the two issues: adding more complexity to the neighbour aggregation slows down training. Typically, the simplest mean or sum pooling suffices, or an attention module can be added to learn which neighbours to attend to most.

% For sequential recommendations, the main issue is that most training sequences are too short. Most solutions involve adding more edges either from other related sequences or from other relations (eg. 2nd hop item relations from a user-item graph). Attention over earlier states in the sequence with respect to the last state is often used to improve prediction of the next item.

% Relevant future work concerns dynamic graphs (how can new nodes or edges be used to update the representations), the reception field (a fixed number of hops will include too wide of a reception field for highly linked nodes compared to relatively inactive or new nodes), and self supervised learning (such as in masked item / edge learning). 

\subsection{RecoBERT: A Catalog Language Model for Text-Based Recommendations \cite{Malkiel2020}}

RecoBERT is an early example from Microsoft of using a pretrained LLM for text-based item recommendations, a pure item-to-item recommender system. Earlier attempts that also incorporated context and/or user-item interactions (eg. \cite{Djuric2015}, \cite{Zheng2017} or \cite{deSouza2018}) trained text embeddings from scratch with a custom network, making that possibly more costly to develop and less general. 

Title, document pairs are input to BERT$_large$ as the sequence [CLS][title tokens][SEP][document tokens], 15\% of which are masked; the output embeddings for the title and document are separately averaged. The cosine distance between the title, document embeddings, $C_{TDM}$ is used in the title-description model (TDM) loss:
\begin{align}
  \begin{split}
  \mathcal{L}_{TDM} = -{1 \over 2}\sum_{i=1}^n & \left(  y_i \log \left(C_{TDM}^i \right) \right. + \\
                                 &  \left. (1-y_i)\log \left( 1- C_{TDM}^i \right) \right)
  \end{split}
\end{align} 
The total loss includes a mask language model component (following \cite{Devlin2019}) for a classification layer maps the BERT [CLS] embedding back to vocabulary space.

For inference, to test the similarity of a candidate title$^\prime$, document$^\prime$ to a known title, document they compute: cosine similarity of titles, cosine similarity of documents and the TDM model cosine distance crossed title, document$^\prime$ and title$^\prime$, document pairs; the sum of which define a similarity metric that they rank by.

They evaluate RecoBERT on two datasets: a wine review catalogue \cite{kagglewine} (with an expert annotated test set\footnote[1]{See the author's github page for this paper, \url{https://github.com/r-papso/recobert}}), and a fashion catalog (no further details provided, presumably proprietary). RecoBERT is trained separately for each: to train on the wine dataset with 120k examples on a system with one NVIDIA V100 32GB GPU. They compare against other sentence embeddings, pre-trained BERT without finetuning, and BERT finetuned to each domain (on the same reviews that RecoBERT trains on but without the TDM). RecoBERT outperformed the other approaches, often by a large margin, as quantified by either mean reciprocal ratio (MMR) or hit ratio (HR) for various top-$k$. 


\subsection{M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems \cite{Cui2022}}



\section{Compare and contrast} 

Unfortunately, the SBERT paper doesn't include "a simple but tough-to-beat baseline for sentence embeddings" \cite{Arora2017} in their comparisons. Strangely, the two papers state fairly different performance for the same embeddings on the same tasks (eg. simple average GloVe embeddings \cite{GloVe} on the STS benchmarks); either the GloVe embeddings used were trained on different corpuses or they're reporting different summaries of results across the subtasks ("a simple..." stated mean across STS tasks and used the \href{https://nlp.stanford.edu/projects/glove/}{glove.840B.300d} embeddings; the SBERT paper doesn't say). Since both state a 5-20 point improvement over unweighted GloVe embedding averages, it would have been very interesting to see how SBERT performs against a better baseline.

Like ColBERT, RecoBERT must to finetuned to each domain but much of the heavy computation can be precomputed and indexed, although the authors did not consider this. RecoBERT uses title, document pairs and train with a cosine loss to discriminate positive vs negative pairs; compare that with the triplets trained on in both SBERT and ColBERT. 

% Point out the similarities and differences of the papers. Do they agree with each other? Are results seemingly in conflict? If the papers address different subtasks, how are they related? (If they are not related, then you may have made poor choices for a lit review...). This section is probably the most valuable for the final project, as it can become the basis for a lit review section.

\section{Future work} 

Many studies suggest that a contrastive loss with triplets performs better; triplet loss works with the embeddings directly rather than forcing them first through bottleneck layers \cite{FaceNet}. RecoBERT should see improvements switching to triplet training from their current (pairwise) constrastive loss.

Furthermore, the efficacy of triplet training relies on finding "good" positive/negative pairs (distinguishable but barely so), though gradient clipping must ease learning from too-hard pairs; in image and speech domains, much attention is given to choosing better pairs \cite{triplepairs}. Adapting these techniques to NLP may improve both SBERT and ColBERT. 


% Make several suggestions for how the work can be extended. Are there open questions to answer? This would presumably include how the papers relate to your final project idea.



\bibliography{project}

% \appendix

% \section{Example Appendix}\label{sec:appendix}

% This is an appendix.

\end{document}
