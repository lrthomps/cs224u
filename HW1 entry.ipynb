{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783dc3bb-ef14-49c8-9f76-a7db8fb5052e",
   "metadata": {},
   "source": [
    "# HW 1 entry\n",
    "by Lara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde680f8-6b93-4f14-8cd5-c6521e0860bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.spatial.distance as d\n",
    "\n",
    "import os\n",
    "DATA_HOME = os.path.join('data', 'wordrelatedness')\n",
    "\n",
    "import vsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c831f4-cc56-4d65-a40c-b139a4bda9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed to get the vocabulary!\n",
    "gigawin5_df = pd.read_csv(\"data/vsmdata/giga_window5-scaled.csv.gz\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d21b4b6-7b5c-4dac-b2c6-5bffe6ce2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\n",
    "    os.path.join(DATA_HOME, \"cs224u-wordrelatedness-dev.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6604ead3-20d5-4a52-b318-7394f2db0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and load this one to make sure we have the entire vocabulary\n",
    "test_df = pd.read_csv(\n",
    "    os.path.join(DATA_HOME, \"cs224u-wordrelatedness-test-unlabeled.csv\"))\n",
    "\n",
    "dev_vocab = list(set(dev_df.word1.values) | set(dev_df.word2.values))\n",
    "test_vocab = list(set(test_df.word1.values) | set(test_df.word2.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68ed88f-fcb2-48d1-943d-f19d39efdaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use GloVe embeddings trained on 840B tokens (bigger training set that the GloVe embeddings included in the course data)\n",
    "# there are words missing but none are in the dev or test set.\n",
    "\n",
    "from torch_autoencoder import TorchAutoencoder\n",
    "import torch.nn as nn\n",
    "\n",
    "embeddings_dict = {}\n",
    "# downloaded from https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "with open(\"data/glove.840B.300d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        vector = np.asarray(values[-300:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "glove_df = pd.DataFrame(embeddings_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4e704dd-ddd9-4331-8d24-d8d7d246e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "# must drop some words from our vocabulary because GloVe didn't tokenize the same way and doesn't have them\n",
    "keep = gigawin5_df.drop(['):', ');', ':(', ':/', \"america\\'s\", \"aren\\'t\", \"children\\'s\", \"city\\'s\", \"company\\'s\", \"couldn\\'t\", \n",
    "                    \"family\\'s\", \"friend\\'s\", \"hadn\\'t\",  '..', ':D',\n",
    "                    \"hasn\\'t\", \"haven\\'t\", \"he\\'d\", \"he\\'ll\", \"he\\'s\", \"here\\'s\", \"husband\\'s\", \"i\\'d\", \"i\\'ll\", \"i\\'m\", \n",
    "                    \"i\\'ve\", \"isn\\'t\", \"it\\'ll\", \"let\\'s\", \"mcdonald\\'s\", \"men\\'s\", \"mother\\'s\", \"people\\'s\", \"she\\'s\", \n",
    "                    \"shouldn\\'t\", \"son\\'s\", \"sunday\\'s\", \"there\\'s\", \"they\\'d\", \"they\\'ll\", \"they\\'re\", \"they\\'ve\", \"today\\'s\", \n",
    "                    \"wasn\\'t\", \"we\\'d\", \"we\\'ll\", \"we\\'re\", \"we\\'ve\", \"weren\\'t\", \"what\\'s\", \"who\\'s\", \"wife\\'s\", \"women\\'s\", \n",
    "                    \"won\\'t\", \"world\\'s\", \"would\\'ve\", \"wouldn\\'t\", \"year\\'s\", \"you\\'d\", \"you\\'ve\",\n",
    "                     \"can\\'t\", \"didn\\'t\", \"doesn\\'t\", \"don\\'t\", \"it\\'s\", \"that\\'s\", \"you\\'ll\", \"you\\'re\"]).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be911880-1853-4086-8c3b-70ba48570e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to regain most of the words that weren't in GloVe in our vocabulary:\n",
    "\n",
    "mapping = {#'):', ');', ':(', ':/', \n",
    "           \"aren\\'t\": 'arent', \"couldn\\'t\": 'couldnt', \"hadn\\'t\": 'hadnt',  \"can\\'t\": 'cannot', \n",
    "                     #'..', ':D',\n",
    "           \"hasn\\'t\": 'hasnt', \"haven\\'t\": 'havent', \n",
    "           \"isn\\'t\": 'isnt', \n",
    "           \"shouldn\\'t\": 'shouldnt', \"they\\'d\": 'theyd', \"they\\'ll\": 'theyll', \"they\\'re\": 'theyre', \"they\\'ve\": 'theyve', \n",
    "           \"wasn\\'t\": 'wasnt',  \n",
    "           \"weren\\'t\": 'werent', \n",
    "           \"would\\'ve\": 'wouldve', \n",
    "           \"wouldn\\'t\": 'wouldnt', \n",
    "           \"you\\'d\": 'youd', \"you\\'ve\": 'youve',\n",
    "           \"didn\\'t\": 'didnt', \"doesn\\'t\": 'doesnt', \n",
    "           \"you\\'ll\": 'youll', \"you\\'re\": 'youre'\n",
    "}\n",
    "\n",
    "combine_possessive = [\"america\\'s\", \"children\\'s\", \"city\\'s\", \"company\\'s\", \"family\\'s\", \"friend\\'s\",\n",
    "                 \"here\\'s\", \"husband\\'s\", \"mcdonald\\'s\", \"men\\'s\", \"mother\\'s\", \"people\\'s\", \"today\\'s\", \n",
    "                \"son\\'s\", \"sunday\\'s\", \"wife\\'s\", \"women\\'s\", \"world\\'s\", \"year\\'s\"]\n",
    "\n",
    "combine_is = [\"he\\'s\",  \"it\\'s\", \"she\\'s\",  \"there\\'s\", \"what\\'s\", \"who\\'s\",  \"that\\'s\"]\n",
    "\n",
    "combine_generic = {\"he\\'d\": ['he', 'would'], \"he\\'ll\": ['he', 'will'],   \"i\\'d\": ['i', 'would'], \n",
    "                   \"i\\'ll\": ['i', 'will'], \"i\\'m\": ['i', 'am'], \"i\\'ve\": ['i', 'have'], \n",
    "                   \"it\\'ll\": ['it', 'will'], \"let\\'s\": ['let', 'us'], \n",
    "                   \"we\\'d\": ['we', 'would'], \"we\\'ll\": ['we', 'will'], \"we\\'re\": ['we', 'are'],\n",
    "                   \"we\\'ve\": ['we', 'have'], \"won\\'t\": ['will', 'not'], \"don\\'t\": ['do', 'not']\n",
    "}\n",
    "emb_mapped = glove_df.loc[mapping.values()]\n",
    "emb_mapped.index = mapping.keys()\n",
    "\n",
    "emb_poss = glove_df.loc[[c.split(\"\\'s\")[0] for c in combine_possessive]] + glove_df.loc[\"'s\"].values\n",
    "emb_poss.index = combine_possessive\n",
    "\n",
    "emb_is = glove_df.loc[[c.split(\"\\'s\")[0] for c in combine_is]] + glove_df.loc[\"is\"].values\n",
    "emb_is.index = combine_is\n",
    "\n",
    "emb_comb = [glove_df.loc[word[0]] + glove_df.loc[word[1]] for word in combine_generic.values()]\n",
    "emb_comb = pd.DataFrame(data=emb_comb, index=combine_generic.keys())\n",
    "\n",
    "emb = pd.concat([glove_df.loc[keep], emb_mapped, emb_poss, emb_is, emb_comb], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd2765bf-7650-4a96-b94f-729fd1e10276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isin(dev_vocab, emb.index).all(), np.isin(test_vocab, emb.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f71ac717-f67e-4448-a3ec-f051463dc45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1 of 10000; error is 6.241227388381958"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x175ae2d60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 1053. Training loss did not improve more than tol=1e-05. Final error is 0.16253660805523396."
     ]
    }
   ],
   "source": [
    "# autoencode the dimension down to 250\n",
    "\n",
    "x = emb.values\n",
    "x = x/x.std(axis=0)\n",
    "ae = TorchAutoencoder(hidden_dim=250, max_iter=10000, \n",
    "                      hidden_activation=nn.Sigmoid()).fit(x)\n",
    "ae_df = pd.DataFrame(ae, index=emb.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd3ccd90-6dd8-471f-a833-ebefc8047ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7736322316655473\n"
     ]
    }
   ],
   "source": [
    "# and evaluate:\n",
    "df, rho = vsm.word_relatedness_evaluation(dev_df, ae_df, distfunc=d.correlation)\n",
    "print(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "934b3e61-0545-469d-aad9-0f65a0e5888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in hindsight, the lost words were never missed, and it's worth seeing if the model does better still without them\n",
    "print(np.isin(dev_vocab, keep).all(), np.isin(test_vocab, keep).all())\n",
    "emb = glove_df.loc[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09c1df10-ad16-4f6b-8e4d-dcce50f4a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1 of 10000; error is 6.234529912471771"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x152e30f90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 1069. Training loss did not improve more than tol=1e-05. Final error is 0.16211374662816525."
     ]
    }
   ],
   "source": [
    "x = emb.values\n",
    "x = x/x.std(axis=0)\n",
    "ae = TorchAutoencoder(hidden_dim=250, max_iter=10000, \n",
    "                      hidden_activation=nn.Sigmoid()).fit(x)\n",
    "ae_df = pd.DataFrame(ae, index=emb.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7116470-9a53-446d-972f-c9a76ce60efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7739386387717364\n"
     ]
    }
   ],
   "source": [
    "df, rho = vsm.word_relatedness_evaluation(dev_df, ae_df, distfunc=d.correlation)\n",
    "print(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe66170-f924-4556-9bc2-fe1ce49ceace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more or less the same, phew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
