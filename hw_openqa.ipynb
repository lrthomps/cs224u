{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0qZfMf4Yreh"
   },
   "source": [
    "# Few-shot OpenQA with ColBERT retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cgpotts/cs224u/blob/master/hw_openqa.ipynb)\n",
    "\n",
    "If colab is opened with this badge, please **save copy to drive** in 'File' menu before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqUPAnWuYrej"
   },
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts and Omar Khattab\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6TRA3gNYrek"
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [Contents](#Contents)\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "    1. [Google Colab set-up](#Google-Colab-set-up)\n",
    "    1. [General set-up](#General-set-up)\n",
    "    1. [Language model set-up](#Language-model-set-up)\n",
    "    1. [ColBERT set-up](#ColBERT-set-up)\n",
    "1. [Language models](#Language-models)\n",
    "    1. [Answerhood](#Answerhood)\n",
    "    1. [Eleuther models from Hugging Face](#Eleuther-models-from-Hugging-Face)\n",
    "    1. [GPT-3](#GPT-3)\n",
    "1. [SQuAD](#SQuAD)\n",
    "    1. [SQuAD dev](#SQuAD-dev)\n",
    "    1. [SQuAD dev sample](#SQuAD-dev-sample)\n",
    "    1. [SQuAD train](#SQuAD-train)\n",
    "1. [Evaluation](#Evaluation)\n",
    "1. [Open QA with no context](#Open-QA-with-no-context)\n",
    "1. [Few-shot QA](#Few-shot-QA)\n",
    "1. [ColBERT](#ColBERT)\n",
    "    1. [ColBERT parameters](#ColBERT-parameters)\n",
    "    1. [ColBERT index](#ColBERT-index)\n",
    "    1. [Search](#Search)\n",
    "    1. [Retrieval evaluation](#Retrieval-evaluation)\n",
    "1. [Zero-shot OpenQA with ColBERT retrieval](#Zero-shot-OpenQA-with-ColBERT-retrieval)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "    1. [Few-shot OpenQA with no context [2 points]](#Few-shot-OpenQA-with-no-context-[2-points])\n",
    "    1. [Few-shot OpenQA [2 points]](#Few-shot-OpenQA-[2-points])\n",
    "    1. [Answer scoring [2 points]](#Answer-scoring-[2-points])\n",
    "    1. [Your original system [3 points]](#Your-original-system-[3-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])\n",
    "1. [Submission instructions](#Submission-instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXqIujoUYrek"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The goal of this homework is to explore few-shot (or, prompt-based) learning in the context of open-domain question answering. This is an exciting area that brings together a number of recent task ideas and modeling innovations.\n",
    "\n",
    "Our core task is __open-domain question answering (OpenQA)__. In this task, all that is given by the dataset is a question text, and the task is to answer that question. By contrast, in modern QA tasks, the dataset provides a text and a gold passage, with a guarantee that the answer will be a substring of the passage. \n",
    "\n",
    "OpenQA is substantially harder than standard QA. The usual strategy is to use a _retriever_ to find passages in a large collection of texts and train a _reader_ to find answers in those passages. This means we have no guarantee that the retrieved passage will contain the answer we need. If we don't retrieve a passage containing the answer, our reader has no hope of succeeding. Although this is challenging, it is much more realistic and widely applicable than standard QA. After all, with the right retriever, an OpenQA system could be deployed over the entire Web.\n",
    "\n",
    "The task posed by this homework is harder even than OpenQA. We are calling this task __few-shot OpenQA__. The defining feature of this task is that the reader is simply a general purpose autoregressive language model. It accepts string inputs (prompts) and produces text in response. It is not trained to answer questions per se, and nothing about its structure ensures that it will respond with a substring of the prompt corresponding to anything like an answer.\n",
    "\n",
    "__Few-shot QA__ (but not OpenQA!) is explored in the famous GPT-3 paper ([Brown et al. 2020](https://arxiv.org/abs/2005.14165)). The authors are able to get traction on the problem using GPT-3, an incredible finding. Our task here – __few-shot OpenQA__ – pushes this even further by retrieving passages to use in the prompt rather than assuming that the gold passage can be used in the prompt. If we can make this work, then it should be a major step towards flexibly and easily deploying QA technologies in new domains.\n",
    "\n",
    "In summary:\n",
    "\n",
    "| Task             | Passage given | Task-specific reader training |Task-specific retriever training  | \n",
    "|-----------------:|:-------------:|:-----------------------------:|:--------------------------------:|\n",
    "| QA               | yes           | yes                           | n/a                              |\n",
    "| OpenQA           | no            | yes                           | maybe                            |\n",
    "| Few-shot QA      | yes           | no                            | n/a                              |\n",
    "| Few-shot OpenQA  | no            | no                            | maybe                            | \n",
    "\n",
    "Just to repeat: your mission (should you choose to accept it!) is to explore the final line in this table. The core notebook and assignment don't address the issue of training the retriever in a task-specific way, but we've given some pointers on this in the context of [the original system question at the bottom of this notebook](#Your-original-system-[3-points]).\n",
    "\n",
    "As usual, this notebook sets up the task and provides starter code. We proceed through a series of approaches:\n",
    "\n",
    "* _Open QA with no context_: the prompt consists of the question, and we just see what comes back. This is not particularly fair to the system since it doesn't unambiguously convey what we want it to do, but it's a start.\n",
    "\n",
    "* _Few-shot QA_: the prompt contains one or more examples formatted so as to indirectly convey what we want the system to do, and it uses the gold passage associated with the example. This is the approach of the GPT-3 paper. It works only for datasets with gold passages.\n",
    "\n",
    "* _Open QA with ColBERT retrieval_: This is roughly as in the previous case, but now we presume no access to a gold passage for our example. Rather, we retrieve a passage from a large corpus using the neural information retrieval model ColBERT.\n",
    "\n",
    "The above examples are followed by some assignment questions aimed at helping you to think creatively about the problem. These problems improve on the above approaches in various ways.\n",
    "\n",
    "All of this culminates in an original system question and some code and unlabeled data (here, just a list of questions) for the bake-off.\n",
    "\n",
    "It is a requirement of the bake-off that a pure autoregressive model be used. In particular, trained QA systems cannot be used at all. See the original system question at the bottom of this message for the full list of allowed models.\n",
    "\n",
    "Note: the models we are working with here are _big_. This poses a challenge that is increasingly common in NLP: you have to pay one way or another. You can pay to use the GPT-3 API, or you can pay to use an Eleuther model on a heavy-duty cluster computer, or you can pay with time by using an Eleuther model on a more modest computer. If none of these options is palatable, you might consider instead doing the [color reference assignment](hw_colors.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKB9zXRBYrel"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JwiISlVYrel"
   },
   "source": [
    "### Google Colab set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbnxdvg7Yrem"
   },
   "source": [
    "We have sought to make this notebook self-contained so that it can easily be run as a Google Colab. If you are running it in Colab, make sure to select a GPU instance. The notebook will run on a CPU-only instance or CPU-only machine, but it should be much faster with GPU support.\n",
    "\n",
    "The following are all installed as part of course set-up, but you'll want to run this cell if you are working in a Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "P2TXnN9HYrem",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c87c8b28-c38f-4fbe-f9ae-6cd6dc0d4b44",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.10.0\n",
      "  Downloading torch-1.10.0-cp39-none-macosx_10_9_x86_64.whl (147.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m147.1/147.1 MB\u001B[0m \u001B[31m15.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from torch==1.10.0) (4.3.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.1\n",
      "    Uninstalling torch-1.10.1:\n",
      "      Successfully uninstalled torch-1.10.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.11.2 requires torch==1.10.1, but you have torch 1.10.0 which is incompatible.\n",
      "torchaudio 0.13.1 requires torch==1.13.1, but you have torch 1.10.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed torch-1.10.0\n",
      "Collecting ujson\n",
      "  Downloading ujson-5.7.0-cp39-cp39-macosx_10_9_x86_64.whl (57 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.4/57.4 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: ujson\n",
      "Successfully installed ujson-5.7.0\n",
      "Requirement already satisfied: transformers in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (4.17.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: filelock in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: click in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from sacremoses->transformers) (8.1.3)\n",
      "Requirement already satisfied: six in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from sacremoses->transformers) (1.2.0)\n",
      "Requirement already satisfied: datasets in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: multiprocess in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pandas in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (1.5.0)\n",
      "Requirement already satisfied: packaging in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (1.23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: dill in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: spacy in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (1.23.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: setuptools in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (62.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: jinja2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: gitpython in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (3.1.30)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from gitpython) (4.0.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.10.0\n",
    "\n",
    "!pip install ujson\n",
    "\n",
    "!pip install transformers\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "!pip install spacy\n",
    "\n",
    "!pip install gitpython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlk-FsfZYrem"
   },
   "source": [
    "If you are indeed on a GPU machine, then run the following to ensure complete CUDA support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fl5wBNCxYrem",
    "outputId": "aacd2379-f6b7-428b-fec7-2dff54c2e9fa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    !pip install cupy-cuda111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej8kZeh6Yren"
   },
   "source": [
    "If the above doesn't work, it might be because you don't have CUDA version 11.1. Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PojRXsuPYren"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    !nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfZEOAL2Yren"
   },
   "source": [
    "and then install the corresponding `cupy-cuda`. See [this table](https://docs.cupy.dev/en/stable/install.html#installing-cupy-from-pypi) for details on which one to install for different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFM54iO7Yren"
   },
   "source": [
    "### General set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hL9AAtTzYren"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from contextlib import nullcontext\n",
    "from collections import namedtuple\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re \n",
    "import string\n",
    "import torch\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNXANb8fYreo"
   },
   "source": [
    "Try to set all the seeds for reproducibility (won't extend to GPT-3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MIvsYoIpYreo"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKp57bQZYrep"
   },
   "source": [
    "The following should install the version of [Faiss](https://github.com/facebookresearch/faiss) that will cooperate with your set-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGUj1O9wYrep",
    "outputId": "ffd634ba-c30c-4d77-c1cf-31533e05a876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu==1.7.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (1.7.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    !pip install faiss-gpu==1.7.0\n",
    "else:\n",
    "    !pip install faiss-cpu==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mp1C-oyYreq"
   },
   "source": [
    "### Language model set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEUY5P8GYreq"
   },
   "source": [
    "To use the GPT-3 API, install the OpenAI library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "4FclKZiuYreq",
    "outputId": "53c0e8ca-e5fb-4bb1-b8be-c963d7517b90",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (0.26.5)\n",
      "Requirement already satisfied: tqdm in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.20->openai) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->openai) (1.8.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5cbE328hYrer"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TKQEIYGDYrer"
   },
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axUPfnNmYrer"
   },
   "source": [
    "### ColBERT set-up\n",
    "\n",
    "Our retriever will be a ColbERT-based model ([Khattab and Zaharia 2020](https://arxiv.org/abs/2004.12832)). ColBERT is a powerful neural information retrieval (Neural IR) model that has proven extremely successful in retrieval applications and as a component in a variety of different systems for OpenQA and other knowledge-intensive tasks (e.g., [Khattab et al. 2021a](https://aclanthology.org/2021.tacl-1.55/); [Khattab et al. 2021b](https://proceedings.neurips.cc/paper/2021/hash/e8b1cbd05f6e6a358a81dee52493dd06-Abstract.html); [Santhanam, Khattab, et al. 2021](https://arxiv.org/abs/2112.01488)).\n",
    "\n",
    "The following will clone the ColBERTv2 repository for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5wP933JYrer",
    "outputId": "1bd1441c-7750-4d22-9519-9f68ff3cce74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ColBERT' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone -b cpu_inference https://github.com/stanford-futuredata/ColBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LWz4f-3Yres",
    "outputId": "11bb2cc3-140e-46d0-d04f-a35f74c97e12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 13:45:19.185346: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/lara.thompson/code/cs224u/ColBERT/colbert/indexing/codecs/residual.py:10: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(not torch.cuda.is_available(), \"cupy must be installed in GPU mode\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'ColBERT/')\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Collection\n",
    "from colbert.searcher import Searcher\n",
    "from utility.utils.dpr import has_answer, DPR_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5ganiuNYres",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Language models\n",
    "\n",
    "In few-shot OpenQA, the language model (LM) must read in a prompt and answer the question posed somewhere in the prompt. We propose two basic strategies:\n",
    "\n",
    "* [EleutherAI](https://www.eleuther.ai/) has released GPT-2-style models in a variety of sizes. These are free to use and easy to use via Hugging Face, and the larger ones very effective for our task, with GPT-J competitive with GPT-3 even though it has only 6B parameters (vs. 145B for GPT-3). The downside here is that the larger models in this family might be very slow and very difficult to work with unless you have access to really impressive GPU hardware. In testing with the free version of Google Colab, we were basically able to do everything we needed to do for the 1.3B parameter model, but the larger one caused too many problems to be viable.\n",
    "\n",
    "* OpenAI has outstanding API access to GPT-3. You can sign up for [a free account](https://beta.openai.com/signup), and, as of this writing, you get US$18 in credit when you sign up. This is more than enough for the current assignment provided you are careful about how much testing you do. The benefits here are that the API is blazingly fast and requires nothing of your computer in terms of GPU support, and you're getting responses from a 145B parameter model that is truly exceptional.\n",
    "\n",
    "Our suggestion is to do basic development with `\"gpt-neo-125M\"`, scale up to `\"gpt-neo-1.3B\"` once you have a sense for what your original system will be like, and then do your final bake-off entry with GPT-3. The functions `run_eleuther` and `run_gpt3` defined below are totally interchangeable, so this kind of development path should be easy to take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fca8-RXjYres",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Answerhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "U6KepnjTYret"
   },
   "outputs": [],
   "source": [
    "def _find_generated_answer(tokens, newline=\"\\n\" ): \n",
    "    \"\"\"Our LMs tend to insert initial newline characters before\n",
    "    they begin generating text. This function ensures that we \n",
    "    properly capture the true first line as the answer while\n",
    "    also ensuring that token probabilities are aligned.\"\"\"        \n",
    "    answer_token_indices = []\n",
    "    char_seen = False            \n",
    "    for i, tok in enumerate(tokens):\n",
    "        # This is the main condition: a newline that isn't an initial\n",
    "        # string of newlines:\n",
    "        if tok == newline and char_seen:\n",
    "            break\n",
    "        # Keep the initial newlines for consistency:\n",
    "        elif tok == newline and not char_seen:\n",
    "            answer_token_indices.append(i)\n",
    "        # Proper tokens:\n",
    "        elif tok != newline:\n",
    "            char_seen = True\n",
    "            answer_token_indices.append(i)\n",
    "    return answer_token_indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klW12GkAYret",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Eleuther models from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "19d0a74b2d134747880402c4e6f0fad0",
      "fcb0f73a2e314a8e935f0802f2f12f31",
      "9905ea5f04f34c5badc797bd6fa07d3a",
      "4a08f6612d9d4a3e98d00926e56415b0",
      "8a1a0c82837e45709424db63bc86ec86",
      "fb7cce9fecc441b1baf720d2dea6dbfc",
      "7086129269a04514aa49001d7e712216",
      "4742b73a39d2444d882aad5b47076911",
      "9dfca6ee005a4222aa369280c4381a7b",
      "7583f3ce599641d08a625337921e017f",
      "e98a1ee4792240b9b7285556d2f3ecbd",
      "47e4b314bc6c435293cb2e5707d49ae1",
      "f208df7109cd475da4f6979e387af2fc",
      "c94c4ffce4cf4f9c93546bbe106bf807",
      "6ceb7c5f67e24c1490ff9d209b2e23dd",
      "36a30e94d4dd426db17841370f46faf1",
      "9c5dd297ecb9495aa82ca68bbec66e1c",
      "142b1dfda9ae4444a1d577d1c66c215a",
      "c83a2859c1e941bca4849872a56baa53",
      "b3990b506419477b824530e11f8a42ef",
      "446124444abf4c739ad28e593f61562b",
      "d5a9ed85f6424d329727e3651094d394",
      "fb95bf53f6ab4ee19b1efeb17c2ea749",
      "a551f75352ac45b3845c61bbd5c3bb3c",
      "1a8214cc9451415483ce785b997032e6",
      "e131d608e26f44f89e1b155fa8042df9",
      "97c7cc3aa1cb438facac717b44861c71",
      "83693e21aeae47b0a5b0ee42eb72a3b9",
      "03d8affdbcfb44c08ccdf60e888d9978",
      "0e16502571d94ea286c1fd32d387e5ea",
      "a093e8d14d6645daae23a16ecd01e423",
      "6b11d0d12a3c42f88defea028bc27545",
      "a5246564505c48ee98d7644deed5a255",
      "7946163e5da84225a8eccd89122b14df",
      "da906492fc8044dfb04a43afa8a7bbac",
      "76ab7a53d0b64bf8a670dfe31bf72422",
      "c0ab47e3313549cd806f5c10d791d291",
      "6b962355e3eb47678013cfd469b57237",
      "f9c217ef31284d95814342521904b45f",
      "133bffae565a49e79a395b08f880b3db",
      "38030650fe094db6b6436fdcf458a97e",
      "1b98baadc95644f5b83cc6bfc7cd8c09",
      "7efc61ef150b4287841c06cc291722c7",
      "38a2caefa5cd4722be6f553a1d9f7e8e",
      "06363edc1de54c15a0a00e104a08d07b",
      "108c1fbcc56443ba96d57133cad11645",
      "eb96d7ee6af840b2af48783ddad1ddb5",
      "f1726b2d78b54c4aa411b7e4eecbf4eb",
      "279a7470d04e4a34b1afdc7d4761c815",
      "47a7719397c249389d3ebc5753ba2acf",
      "da6c76d121fa4e3ca3d85bc0dcde6212",
      "40db9d36656c46238c5ed87adf8cb4ec",
      "e927e8b18836481cb41c7117c0dcf3c4",
      "c99ffa80225b4a09ad4e7570b6c4e672",
      "6c7c0e3d3a844558b12dcdb952e76920",
      "121976cff01349459b2909033f05c576",
      "a8f1b2d780d74e4382e14974537b1e0b",
      "7211cffdbc0a4ae1a9f894a98cc65b03",
      "577af7ffedd8430da0870d4a9f773056",
      "5616a0f10f344b499c9b9e2f2d4f97b9",
      "cdb7e47723fe45e6845a02ad85871d44",
      "5d67fd950c8e4732a32250c3decb9aef",
      "e0d37a2cda934241b195138780fbe067",
      "9ef856a58be54b9fa8b5c610051c25cf",
      "23bb67fe91d644469d5be8b442b2e3b6",
      "264be22c995f407ab22fb4605601f7c3"
     ]
    },
    "id": "1-FkbTaUYret",
    "outputId": "f42b7a86-c3bb-4164-92bf-de57520f2bdf"
   },
   "outputs": [],
   "source": [
    "# \"gpt-neo-125M\" \"gpt-neo-1.3B\" \"gpt-neo-2.7B\" \"gpt-j-6B\"\n",
    "eleuther_model_name = \"gpt-neo-125M\"\n",
    "\n",
    "eleuther_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"EleutherAI/{eleuther_model_name}\", \n",
    "    padding_side=\"left\", \n",
    "    padding='longest', \n",
    "    truncation='longest_first', max_length=2000)\n",
    "eleuther_tokenizer.pad_token = eleuther_tokenizer.eos_token\n",
    "\n",
    "eleuther_model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"EleutherAI/{eleuther_model_name}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eleuther_model = eleuther_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rEo9YKwNYret"
   },
   "outputs": [],
   "source": [
    "def run_eleuther(prompts, temperature=0.1, top_p=0.95, **generate_kwargs): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompts : iterable of str\n",
    "    temperature : float\n",
    "        It seems best to set it low for this task!\n",
    "    top_p : float\n",
    "       \n",
    "    For options for `generate_kwargs`, see:\n",
    "    \n",
    "    https://huggingface.co/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate\n",
    "    \n",
    "    Options that are likely to be especially relevant include \n",
    "    `temperature`, `length_penalty`, and the parameters that\n",
    "    determine the decoding strategy. With `num_return_sequences > 1`,\n",
    "    the default parameters in this function do multinomial sampling.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts\n",
    "    \n",
    "    {\"prompt\": str, \n",
    "     \"generated_text\": str, \"generated_tokens\": list of str, \"generated_probs\": list of float,\n",
    "     \"answer\": str, \"answer_tokens\": list of str, \"answer_probs\": list of float\n",
    "    }\n",
    "         \n",
    "    \"\"\"\n",
    "    prompt_ids = eleuther_tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        # Automatic mixed precision if possible.\n",
    "        with torch.cuda.amp.autocast() if torch.cuda.is_available() else nullcontext():\n",
    "            model_output = eleuther_model.generate(\n",
    "                prompt_ids,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,           \n",
    "                max_new_tokens=24,\n",
    "                num_return_sequences=1,                \n",
    "                pad_token_id=eleuther_tokenizer.eos_token_id, \n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                **generate_kwargs)\n",
    "        \n",
    "    # Converting output scores using the helpful recipe here:\n",
    "    # https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n",
    "    gen_ids = model_output.sequences[:, prompt_ids.shape[-1] :]\n",
    "    gen_probs = torch.stack(model_output.scores, dim=1).softmax(-1)\n",
    "    gen_probs = torch.gather(gen_probs, 2, gen_ids[:, :, None]).squeeze(-1)\n",
    "    \n",
    "    # Generated texts, including the prompts:\n",
    "    gen_texts = eleuther_tokenizer.batch_decode(\n",
    "        model_output.sequences, skip_special_tokens=True)\n",
    "    \n",
    "    data = []     \n",
    "    iterator = zip(prompts, gen_ids, gen_texts, gen_probs)    \n",
    "    for prompt, gen_id, gen_text, gen_prob in iterator:       \n",
    "        gen_tokens = eleuther_tokenizer.convert_ids_to_tokens(gen_id)\n",
    "        generated_text = gen_text[len(prompt): ]\n",
    "        gen_prob = [float(x) for x in gen_prob.cpu().numpy()] # float for JSON storage\n",
    "        ans_indices = _find_generated_answer(gen_tokens, newline=\"Ċ\")\n",
    "        answer_tokens = [gen_tokens[i] for i in ans_indices]\n",
    "        answer_probs = [gen_prob[i] for i in ans_indices]\n",
    "        answer = \"\".join(answer_tokens).replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")                                       \n",
    "        data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": generated_text,\n",
    "            \"generated_tokens\": gen_tokens,\n",
    "            \"generated_probs\": gen_prob,\n",
    "            \"generated_answer\": answer,\n",
    "            \"generated_answer_probs\": answer_probs,\n",
    "            \"generated_answer_tokens\": answer_tokens})                        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiN04KynYreu",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5t5HTfRkYreu"
   },
   "outputs": [],
   "source": [
    "def run_gpt3(prompts, engine=\"text-babbage-001\", temperature=0.1, top_p=0.95, **gpt3_kwargs):\n",
    "    \"\"\"To use this function, sign up for an OpenAI account at\n",
    "        \n",
    "    https://beta.openai.com/signup\n",
    "    \n",
    "    That should give you $18 in free credits, which is more than enough\n",
    "    for this assignment assuming you are careful with testing.\n",
    "    \n",
    "    Once your account is set up, you can get your API key from your \n",
    "    account dashboard and paste it in below as the value of \n",
    "    `openai.api_key`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prompts : iterable of str\n",
    "    engine : str\n",
    "        This has to be one of the models whose name begins with \"text\".\n",
    "        The \"instruct\" class of models can't be used, since they seem\n",
    "        to depend on some kinds of QA-relevant supervision.        \n",
    "        For options, costs, and other details: \n",
    "        https://beta.openai.com/docs/engines/gpt-3                \n",
    "    temperature : float\n",
    "        It seems best to set it low for this task!\n",
    "    top_p : float\n",
    "        \n",
    "    For information about values for `gpt3_kwargs`, see\n",
    "    \n",
    "    https://beta.openai.com/docs/api-reference/completions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts   \n",
    "    \n",
    "    \"\"\"\n",
    "    # Fill this in with the value from your OpenAI account. First\n",
    "    # verify that your account is set up with a spending limit that\n",
    "    # you are comfortable with. If you just opened your account,\n",
    "    # you should have $18 in credit and so won't need to supply any\n",
    "    # payment information.\n",
    "    openai.api_key = API_KEY\n",
    "    \n",
    "    \n",
    "    assert engine.startswith(\"text\"), \\\n",
    "        \"Please use an engine whose name begins with 'text'.\"\n",
    "        \n",
    "    response = openai.Completion.create(\n",
    "        engine=engine,       \n",
    "        prompt=prompts,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=32,\n",
    "        echo=False,   # This function will not work\n",
    "        logprobs=1,   # properly if any of these\n",
    "        n=1,          # are changed!\n",
    "        **gpt3_kwargs)\n",
    "    \n",
    "    # From here, we parse each example to get the values\n",
    "    # we need:\n",
    "    data = []\n",
    "    for ex, prompt in zip(response[\"choices\"], prompts):\n",
    "        tokens = ex[\"logprobs\"][\"tokens\"]\n",
    "        logprobs = ex[\"logprobs\"][\"token_logprobs\"]        \n",
    "        probs = list(np.exp(logprobs))\n",
    "        if \"<|endoftext|>\" in tokens:\n",
    "            end_i = tokens.index(\"<|endoftext|>\")\n",
    "            tokens = tokens[ : end_i]  # This leaves off the \"<|endoftext|>\"\n",
    "            probs = probs[ : end_i]    # token -- perhaps dubious.\n",
    "        ans_indices = _find_generated_answer(tokens)\n",
    "        answer_tokens = [tokens[i] for i in ans_indices]\n",
    "        answer_probs = [probs[i] for i in ans_indices]\n",
    "        answer = \"\".join(answer_tokens)        \n",
    "        data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": ex[\"text\"],\n",
    "            \"generated_tokens\": tokens,\n",
    "            \"generated_probs\": probs,\n",
    "            \"generated_answer\": answer,\n",
    "            \"generated_answer_tokens\": answer_tokens,\n",
    "            \"generated_answer_probs\": answer_probs})\n",
    "        \n",
    "    return data             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGK3hCs9Yrev",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SQuAD\n",
    "\n",
    "Our core development dataset is [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/). We chose this dataset because it is well-known and widely used, and it is large enough to support lots of meaningful development work, without, though, being so large as to require lots of compute power. It is also useful that it has gold passages supporting the standard QA formulation, so we can see how well our LM performs with an \"oracle\" retriever that always retrieves the gold passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295,
     "referenced_widgets": [
      "fedae0a67e9a4f249d9d2ab527d973e2",
      "280ab82e1c5d4dd8abc4bd15e58034af",
      "8c59fe3638c344748d6eed8206e8975d",
      "5f72f99cd4094dc292146e35a4d67767",
      "35e94aa9d357405780025da79e3691af",
      "87991a91694f456f9318dc5afccec2c9",
      "32e27633a7054f17af4dca4ed5e43279",
      "1acc184682164c08aee48278ae399814",
      "368e5fc79b9e4ff9b4fe2e512947d677",
      "72570da75be34d37b32be2dae93222ec",
      "91859e90f7784956bdc1d40a984d2c5b",
      "bb922ea5e97a44d8b0e94d9f76773167",
      "32fbedf574754ccc85c4b59d83f743f4",
      "439bd70a6fda44b993051827f6f072e3",
      "b704aad9cc6e474f988e8966ca179da1",
      "dcc8bd4d5b7242ee8c47586cb73c7c0a",
      "5c33f1097177468aa401020bac71a5be",
      "cef06e938024471cabfcfdad4411b961",
      "830d66f3b3e64b6f8c86ba775539672e",
      "d370b83b677d454ea1c81470671443fd",
      "8639b41657e04b70ac4f3f3ca517a035",
      "5071e184457e42bf91a84cc5ea827c19",
      "88fe4f76341e461abf81f2bf7596913b",
      "5c12675e10044c6db66daf30f4d4f948",
      "38cc26500d7f45909637c46e8a676cb4",
      "6bc69367538f4af683b8a21b3dc59268",
      "2d637c205b844d189f5e4232cb1ffda2",
      "6510644f7d0545e183053878c6d6c161",
      "73cdc7a035d544618e66ed586d7b9649",
      "7018804c8581472298d7c8f24beda197",
      "62196b4464bc48a59fa20f1f28f0fe7b",
      "c28e93ddbc77422484ef2de085794312",
      "b55b8143e71c4a2cbca7d399fd179797",
      "8c52264dbf2f4cca86e05c112423b536",
      "d14af2ecb8664d26b0792eb47791741b",
      "52cd2dfb427b49feb9546ae276438cc4",
      "f007de44513a41bf95f9f8b861cdaa6f",
      "f0d1f5455d934205b20d0b635b746f5d",
      "ac576998bfc940e7af4490bae2ad934f",
      "4b007c951e474e3db19aa16ca96c16a6",
      "40c60aa79e644772944b4ffdfd6f7c5a",
      "9fd4ae98811341e285ccf84fc7bb73bb",
      "73b8aa0fe175415a9864d017c5f88682",
      "c0f59c1508c64a02bdbff82ed569b800",
      "d556c1920c6f493996a7ba6b0d5fb946",
      "2974fef15c3841baba7035eb6d360156",
      "0ac869bb35124b39be5bdad7fc78473f",
      "a7e04c08f551443481414a0dd619006e",
      "b40610e2ebb74029bddea0c818532604",
      "c05c539d546447a5882191f1f1e676a9",
      "86a5ca66daa64c4b888c375b0d014099",
      "acfc9aa4af0b4ba2a397789fdc63d4cb",
      "254c594f27134d9b9e18736b7f8d0d57",
      "c3a64df8553f4deea779b13a44233546",
      "ad4e6f09e7b342a59235965668fc34a6",
      "483121c04c514094ba89858a569144d5",
      "872202218d51454faa1211c35a96dc40",
      "b58f664dd2644d43946aec35857d2b11",
      "41f19619d7e347569818bf786673c468",
      "3fb7649992084382b327cb61976124ea",
      "ce46384c19e84fd99447ede08bbaf243",
      "03b85fa378584208a26436b2c65c62a8",
      "c760fb9137f44be789276235d2ec552b",
      "dc4900955f81453f98a10ea9b74b983c",
      "84da01f23908412b9daef0a8cbcf6337",
      "7efeb5f13634479391cffa9a6f9bea49",
      "06c3d71451a44147892cf77cbee7704e",
      "f4047dc7f8fe40e3858efacfe3fa9db3",
      "36a8a57e71b54bf297e8dda3043ff966",
      "78f4d0554a3748328f99d69a22eaa482",
      "90cb7850489648eab6264dd813719a9c",
      "2e25e5700521472e83e6888067336682",
      "ab24a9d6b7a84981ac9d335ff571f7e5",
      "2fe2c9cf13de4b968b4f318b221aa7dc",
      "88c02342d65b4b8f9ee1cd25e4ea15ac",
      "8ae88195643b40bda346711154a9c7bd",
      "1625a303748840aea84cd884c8453578",
      "d833a94f93074a4085fb2de3583037f5",
      "ae5e15d70bf64cde9d8f2caefa2c27f2",
      "2badb7d424fa46f6b88b3e7acaf6ab48",
      "42c3232aaad049ff92e95957128e2f2d",
      "9c27bd5813724a2c835e62854b76c2b8",
      "ed33408a0cad4a46a8366263bdf81bfa",
      "5969f13e33594392ad1a9245313fd91f",
      "3b55afcc50d44f5bbdc3b2c6d06bbc87",
      "d84e03f9117448ef970b930aa77fc968",
      "467d2cd91db246ffaba3af644a47ff1d",
      "cb5b0f438ff7492e88d9fe4cc9184e9d",
      "3538e755160e478f8cf1262e14f2c484",
      "6b096d4bd8d4497c98176ccf9fe26a13",
      "e7b24e1fa266446cb6bb61ec067fa412",
      "3eccf8ba2e3f4ee4b3a90a1cd89e43b1",
      "6af64542f9d547afb656f14a87e6ba03",
      "62639c9b27be44ee83f9b054f76f651b",
      "42b97283bd4d4cdc92fed717c2513919",
      "f86f33b86afb468c8758174771d676c4",
      "304a8303bd0044e6aa53335b6ab30d5a",
      "d958f867d5eb40379753a14982302d08",
      "76b9355cce89436892b959f00b1cf702"
     ]
    },
    "id": "YQ_do58EYrev",
    "outputId": "93464cd2-7658-40ac-f411-90cc3b7ba698"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset squad (/Users/lara.thompson/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c92b65285374f4a8a0977d4e72c0416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbCUz66GYrev"
   },
   "source": [
    "The following utility just reads a SQuAD split in as a list of `SquadExample` instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "B9-0hkxgYrew"
   },
   "outputs": [],
   "source": [
    "SquadExample = namedtuple(\"SquadExample\",  \"id title context question answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "g2gt0dkeYrew"
   },
   "outputs": [],
   "source": [
    "def get_squad_split(squad, split=\"validation\"):\n",
    "    \"\"\"\n",
    "    Use `split='train'` for the train split.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of SquadExample named tuples with attributes\n",
    "    id, title, context, question, answers\n",
    "    \n",
    "    \"\"\"    \n",
    "    fields = squad[split].features\n",
    "    data = zip(*[squad[split][field] for field in fields])\n",
    "    return [SquadExample(eid, title, context, question, answers[\"text\"]) \n",
    "            for eid, title, context, question, answers in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SQuAD dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NatnOLsDYrew"
   },
   "outputs": [],
   "source": [
    "squad_dev = get_squad_split(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jb-ZrSzoYrew",
    "outputId": "c3d9b481-2620-434a-cd7b-43c90fc2d096"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SquadExample(id='56be4db0acb8001400a502ec', title='Super_Bowl_50', context='Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', question='Which NFL team represented the AFC at Super Bowl 50?', answers=['Denver Broncos', 'Denver Broncos', 'Denver Broncos'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dev[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQuAD dev sample\n",
    "\n",
    "We'll use this fixed but presumably quite random set of examples for exploration and system development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_exs = sorted(squad_dev, key=lambda x: hash(x.id))[: 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQuAD train\n",
    "\n",
    "To build few-shot prompts, we will often sample SQuAD train examples, so we load that split here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = get_squad_split(squad, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZpXMk-0Yrew",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Our evaluation protocols are the standard ones for SQuAD and related tasks: exact match of the answer (EM) and token-level F1.\n",
    "\n",
    "We say further that the predicted answer is the first line of generated text after the prompt.\n",
    "\n",
    "The following evaluation code is taken from the [apple/ml-qrecc](https://github.com/apple/ml-qrecc/blob/main/utils/evaluate_qa.py) repository. It performs very basic string normalization before doing the core comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "nHHntDSSYrew",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s: str) -> List[str]:\n",
    "    \"\"\"Normalize string and split string into tokens.\"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_gold: str, a_pred: str) -> int:\n",
    "    \"\"\"Compute the Exact Match score.\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1_from_tokens(gold_toks: List[str], pred_toks: List[str]) -> float:\n",
    "    \"\"\"Compute the F1 score from tokenized gold answer and prediction.\"\"\"\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_f1(a_gold: str, a_pred: str) -> float:\n",
    "    \"\"\"Compute the F1 score.\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    return compute_f1_from_tokens(gold_toks, pred_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJo6C7pgYrex"
   },
   "source": [
    "The following is our general evaluation function. We will make extensive use of it to evaluate different systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bJHSxnA6Yrex"
   },
   "outputs": [],
   "source": [
    "def evaluate(examples, prompts, gens):\n",
    "    \"\"\"Generic evalution function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples: iterable of `SquadExample` instances\n",
    "    prompts: list of str\n",
    "    preds: list of LM-generated texts to evaluate as answers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys \"em_per\", \"macro_f1\", \"examples\", where\n",
    "    each \"examples\" value is a dict\n",
    "    \n",
    "    \"\"\"        \n",
    "    results = []\n",
    "    for ex, prompt, gen in zip(examples, prompts, gens):\n",
    "        answers = ex.answers\n",
    "        pred = gen['generated_answer']\n",
    "        # The result is the highest EM from the available answer strings:\n",
    "        em = max([compute_exact(ans, pred) for ans in answers])\n",
    "        f1 = max([compute_f1(ans, pred) for ans in answers])\n",
    "        gen.update({\n",
    "            \"id\": ex.id, \n",
    "            \"question\": ex.question, \n",
    "            \"prediction\": pred, \n",
    "            \"answers\": answers, \n",
    "            \"em\": em,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "        results.append(gen)\n",
    "    data = {}        \n",
    "    data[\"macro_f1\"] = np.mean([d['f1'] for d in results])\n",
    "    data[\"em_per\"] = sum([d['em'] for d in results]) / len(results)\n",
    "    data[\"examples\"] = results\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj170C1PYrex"
   },
   "source": [
    "Here is a highly simplified example to help make the logic behind `evaluate` clearer:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bgFXLK3Yrex",
    "outputId": "e941a6f7-e326-4b26-e91e-8c0ccd11a0a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_f1': 1.0,\n",
       " 'em_per': 1.0,\n",
       " 'examples': [{'generated_answer': 'NLU',\n",
       "   'generated_text': 'NLU\\nWho am I?',\n",
       "   'id': '0',\n",
       "   'question': 'What is the course to take?',\n",
       "   'prediction': 'NLU',\n",
       "   'answers': ['NLU', 'CS224u'],\n",
       "   'em': 1,\n",
       "   'f1': 1.0}]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = namedtuple(\"SquadExample\",  \"id title context question answers\")\n",
    "\n",
    "examples = [\n",
    "    ex(\"0\", \"CS224u\", \n",
    "       \"The course to take is NLU!\", \n",
    "       \"What is the course to take?\", \n",
    "       [\"NLU\", \"CS224u\"])]\n",
    "\n",
    "prompts = [\"Dear model, Please answer this question!\\n\\nQ: What is the course to take?\\n\\nA:\"]\n",
    "\n",
    "gens = [{\"generated_answer\": \"NLU\", \"generated_text\": \"NLU\\nWho am I?\"}]\n",
    "\n",
    "evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHZHve9NYrex"
   },
   "source": [
    "The bake-off uses `macro_f1` as the primary metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3LQv2lbYrex",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Open QA with no context\n",
    "\n",
    "We now have all the pieces we need to begin building few-shot OpenQA systems. Our first system is the simplest and most naive: we simply feed the question text in as the prompt and hope that the model provides an answer as the first line of its generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "swF4V0ngYrex"
   },
   "outputs": [],
   "source": [
    "def evaluate_no_context(examples, gen_func=run_eleuther, batch_size=20):\n",
    "    prompts = [] \n",
    "    gens = []\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        ps = [ex.question for ex in examples[i: i+batch_size]]\n",
    "        gs = gen_func(ps)        \n",
    "        prompts += ps\n",
    "        gens += gs    \n",
    "    return evaluate(examples, prompts, gens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPuq5nzDYrex",
    "outputId": "cea1edaf-728b-4037-abbc-b58a2d82bfbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030245720756506625\n",
      "CPU times: user 58 s, sys: 7.27 s, total: 1min 5s\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nocontext_results = evaluate_no_context(dev_exs)\n",
    "\n",
    "print(nocontext_results['macro_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "lWi1r1-oYrey",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9ffe19d9-ddb1-4a91-9950-06ae521a2a77",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'API_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:1\u001B[0m\n",
      "Cell \u001B[0;32mIn [40], line 6\u001B[0m, in \u001B[0;36mevaluate_no_context\u001B[0;34m(examples, gen_func, batch_size)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(examples), batch_size):\n\u001B[1;32m      5\u001B[0m     ps \u001B[38;5;241m=\u001B[39m [ex\u001B[38;5;241m.\u001B[39mquestion \u001B[38;5;28;01mfor\u001B[39;00m ex \u001B[38;5;129;01min\u001B[39;00m examples[i: i\u001B[38;5;241m+\u001B[39mbatch_size]]\n\u001B[0;32m----> 6\u001B[0m     gs \u001B[38;5;241m=\u001B[39m \u001B[43mgen_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mps\u001B[49m\u001B[43m)\u001B[49m        \n\u001B[1;32m      7\u001B[0m     prompts \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m ps\n\u001B[1;32m      8\u001B[0m     gens \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m gs    \n",
      "Cell \u001B[0;32mIn [29], line 40\u001B[0m, in \u001B[0;36mrun_gpt3\u001B[0;34m(prompts, engine, temperature, top_p, **gpt3_kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m\"\"\"To use this function, sign up for an OpenAI account at\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03mhttps://beta.openai.com/signup\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;124;03m\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Fill this in with the value from your OpenAI account. First\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# verify that your account is set up with a spending limit that\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# you are comfortable with. If you just opened your account,\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# you should have $18 in credit and so won't need to supply any\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# payment information.\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m openai\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m \u001B[43mAPI_KEY\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m engine\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m), \\\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease use an engine whose name begins with \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     46\u001B[0m response \u001B[38;5;241m=\u001B[39m openai\u001B[38;5;241m.\u001B[39mCompletion\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m     47\u001B[0m     engine\u001B[38;5;241m=\u001B[39mengine,       \n\u001B[1;32m     48\u001B[0m     prompt\u001B[38;5;241m=\u001B[39mprompts,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     54\u001B[0m     n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,          \u001B[38;5;66;03m# are changed!\u001B[39;00m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgpt3_kwargs)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'API_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nocontext_results_gpt3 = evaluate_no_context(dev_exs, gen_func=run_gpt3)\n",
    "\n",
    "print(nocontext_results_gpt3['macro_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csAimDMGYrey",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Few-shot QA\n",
    "\n",
    "The above formulation is not especially fair to our model, since it doesn't convey anything about the intended structure of the prompt. We want the model to give us an answer to the input question, but we didn't specify that goal unambiguously. Perhaps we were looking for commentary on the question, or a count of the number of tokens it contains, or a passage containing the question string, or something else entirely.\n",
    "\n",
    "In few-shot QA, we construct a prompt that is intended to convey our intentions more clearly. The first part of the prompt gives some examples of what we want, and the final part provides the set-up for our actual question. In the current formulation, we assume access to the gold passage. For example, if our example of interest is\n",
    "\n",
    "```\n",
    "Title: CS224u\n",
    "\n",
    "Background: The course to take is NLU!\n",
    "\n",
    "Q: What is the course to take?\n",
    "```\n",
    "\n",
    "with gold answer ```NLU```, then we would create a prompt with, say, 2 additional examples preceding this, to yield a full prompt like this:\n",
    "\n",
    "```\n",
    "Title: Pragmatics\n",
    "\n",
    "Background: Pragmatics is the study of language use.\n",
    "\n",
    "Q: What is pragmatics?\n",
    "\n",
    "A: The study of language use\n",
    "\n",
    "Title: Bert\n",
    "\n",
    "Background: Bert is a Muppet who is lives with Ernie.\n",
    "\n",
    "Q: Who is Bert?\n",
    "\n",
    "A: Bert is a  Muppet\n",
    "\n",
    "Title: CS224u\n",
    "\n",
    "Background: The course to take is NLU!\n",
    "\n",
    "Q: What is the course to take?\n",
    "\n",
    "A:\n",
    "```\n",
    "This is essentially the formulation used in the GPT-3 paper for SQuAD. The context examples are drawn randomly from the SQuAD train set. We will adopt this same protocol for now. (You might revisit this in the context of your original system.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "lhsl9yvHYrey"
   },
   "outputs": [],
   "source": [
    "def build_few_shot_qa_prompt(ex, squad_train, n_context=2, joiner=\"\\n\\n\"):\n",
    "    segs = []\n",
    "    train_exs = random.sample(squad_train, k=n_context)    \n",
    "    for t in train_exs:\n",
    "        segs += [\n",
    "            f\"Title: {t.title}\",\n",
    "            f\"Background: {t.context}\",\n",
    "            f\"Q: {t.question}\",\n",
    "            f\"A: {t.answers[0]}\"\n",
    "        ]\n",
    "    segs += [\n",
    "        f\"Title: {ex.title}\",\n",
    "        f\"Background: {ex.context}\",\n",
    "        f\"Q: {ex.question}\",\n",
    "        f\"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzLghzI5Yrez"
   },
   "source": [
    "Here's the sort of output we get with `n_context=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEuVae4xYrez",
    "outputId": "1fc52cb8-1fa7-4d84-c911-4e3f2cafd682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Modern_history\n",
      "\n",
      "Background: In the Pre-Modern era, many people's sense of self and purpose was often expressed via a faith in some form of deity, be that in a single God or in many gods. Pre-modern cultures have not been thought of creating a sense of distinct individuality, though. Religious officials, who often held positions of power, were the spiritual intermediaries to the common person. It was only through these intermediaries that the general masses had access to the divine. Tradition was sacred to ancient cultures and was unchanging and the social order of ceremony and morals in a culture could be strictly enforced.\n",
      "\n",
      "Q: Through who did the masses have access to the divine?\n",
      "\n",
      "A: spiritual intermediaries\n",
      "\n",
      "Title: Doctor_Who\n",
      "\n",
      "Background: The most frequent musical contributor during the first 15 years was Dudley Simpson, who is also well known for his theme and incidental music for Blake's 7, and for his haunting theme music and score for the original 1970s version of The Tomorrow People. Simpson's first Doctor Who score was Planet of Giants (1964) and he went on to write music for many adventures of the 1960s and 1970s, including most of the stories of the Jon Pertwee/Tom Baker periods, ending with The Horns of Nimon (1979). He also made a cameo appearance in The Talons of Weng-Chiang (as a Music hall conductor).\n",
      "\n",
      "Q: Who was the most frequent musical contributor to Doctor Who in the first 15 years of the show?\n",
      "\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(build_few_shot_qa_prompt(dev_exs[0], squad_train, n_context=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "VUwgM625Yrez"
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot_qa(examples, squad_train, gen_func=run_eleuther, batch_size=20, n_context=2):\n",
    "    prompts = []\n",
    "    gens = []\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = examples[i: i+batch_size]\n",
    "        ps = [build_few_shot_qa_prompt(ex, squad_train, n_context=n_context) for ex in batch]        \n",
    "        gs = gen_func(ps)       \n",
    "        prompts += ps\n",
    "        gens += gs\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UelblRFCYrez",
    "outputId": "0587385d-207a-47bb-d363-5b33c3a20920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05445034204465673\n",
      "CPU times: user 13min 59s, sys: 2min 23s, total: 16min 23s\n",
      "Wall time: 8min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "few_shot_qa_results = evaluate_few_shot_qa(dev_exs, squad_train, n_context=1)\n",
    "\n",
    "print(few_shot_qa_results['macro_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fdPsEbmYrez",
    "outputId": "be6e22d2-7655-4c98-89f9-a0fe81d204d8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "few_shot_qa_results_gpt3 = evaluate_few_shot_qa(\n",
    "    dev_exs, squad_train, n_context=1, gen_func=run_gpt3)\n",
    "\n",
    "print(few_shot_qa_results_gpt3['macro_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfvVdsGrYre0",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ColBERT\n",
    "\n",
    "It's now just a short step to our core task, few-shot OpenQA. We just need to give up our beloved gold passage and instead try to retrieve the right passage or passages from a corpus. \n",
    "\n",
    "The first step is instantiating the ColBERT retriever and loading in an index. Our ColBERT retriever was initially trained on MS MARCO, and we have pre-indexed a collection of 100K documents that we know to be well-aligned with SQuAD and with the dataset used for the bake-off assessment. (See [the original system question](#Your-original-system-[3-points]) for tips on creating your own index.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_home = os.path.join(\"experiments\", \"notebook\", \"indexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFYxJPpuYre0",
    "tags": []
   },
   "source": [
    "### ColBERT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tnUU2UHYre0",
    "outputId": "9d7431a0-7f69-4549-960f-56f8575a8e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Failed to open the file data/openqa/: Is a directory\n",
      "  0  387M    0 16111    0     0  74761      0  1:30:29 --:--:--  1:30:29 76719\n",
      "curl: (23) Failure writing output to destination\n",
      "tar: Error opening archive: Failed to open 'data/openqa/colbertv2.0.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(\"data\", \"openqa\", \"colbertv2.0.tar.gz\")):\n",
    "    !mkdir -p data/openqa\n",
    "    # ColBERTv2 checkpoint trained on MS MARCO Passage Ranking (388MB compressed)\n",
    "    !curl https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -o data/openqa/\n",
    "    !tar -xvzf data/openqa/colbertv2.0.tar.gz -C data/openqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjZ1hZJnYre0"
   },
   "source": [
    "If something went wrong with the above, you can just download the file https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz, unarchive it, and move the resulting `colbertv2.0` directory into the `data/openqa` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzRjL61eYre0",
    "tags": []
   },
   "source": [
    "### ColBERT index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "jD0U5Outa9HU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "tar: Error opening archive: Failed to open 'experiments/notebook/indexes/cs224u.collection.2bits.tgz'\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(index_home, \"cs224u.collection.2bits.tgz\")):\n",
    "    !wget https://web.stanford.edu/class/cs224u/data/cs224u.collection.2bits.tgz -P experiments/notebook/indexes\n",
    "    !tar -xvzf experiments/notebook/indexes/cs224u.collection.2bits.tgz -C experiments/notebook/indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61ergQcQYre0"
   },
   "source": [
    "If something went wrong with the above, download the file https://web.stanford.edu/class/cs224u/data/cs224u.collection.2bits.tgz, unarchive it, and move the resulting `cs224u.collection.2bits` directory into the `experiments/notebook/indexes` directory (which you will probably need to create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "XtEGC6MyYre0",
    "outputId": "93164bc8-b9da-4893-8b2f-d5eeca0403a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 20, 13:57:40] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 125,563 passages'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = os.path.join(index_home, \"cs224u.collection.2bits\", \"cs224u.collection.tsv\")\n",
    "\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "BixmAYizYre0"
   },
   "outputs": [],
   "source": [
    "index_name = \"cs224u.collection.2bits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our `searcher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6wYpChzYre1",
    "outputId": "1eb08be0-a66c-4414-d69b-32733d74631b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 20, 13:57:43] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 20, 13:57:45] #> Building the emb2pid mapping..\n",
      "[Feb 20, 13:57:46] len(self.emb2pid) = 14968345\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(experiment='notebook')):\n",
    "    searcher = Searcher(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dex5KPUTYre1",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Search\n",
    "\n",
    "Now that the index is loaded, you can do searches over it. The index is limited, but retrieval is very solid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-v2jKaR8Yre1",
    "outputId": "6ae9e5ee-8a1d-4d00-9925-1dbf47d21412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> linguistics\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . linguistics, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1, 15397,   102,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[1]\t24.2\t Linguistics | representation and function of language in the mind; neurolinguistics, which studies language processing in the brain; and language acquisition, which investigates how children and adults acquire a particular language. This is in regards to how children adapt to language, how they learn language, and what influences their ability to learn language. Linguistics also includes non-formal approaches to the study of other aspects of human language, such as social, cultural, historical and political factors. The study of cultural discourses and dialects is the domain of sociolinguistics, which looks at the relation between linguistic variation and social structures, as well as that\n",
      "\t[2]\t24.1\t Linguistics | concerned with how the mind creates meaning through language, and not with the use of language as a tool of communication. Historical linguists study the history of specific languages as well as general characteristics of language change. The study of language change is also referred to as \"diachronic linguistics\" (the study of how one particular language has changed over time), which can be distinguished from \"synchronic linguistics\" (the comparative study of more than one language at a given moment in time without regard to previous stages). Historical linguistics was among the first sub-disciplines to emerge in linguistics, and was the\n",
      "\t[3]\t23.6\t Linguistics | of discourse analysis, which examines the structure of texts and conversations. Research on language through historical and evolutionary linguistics focuses on how languages change, and on the origin and growth of languages, particularly over an extended period of time. Corpus linguistics takes naturally occurring texts and studies the variation of grammatical and other features based on such corpora. Stylistics involves the study of patterns of style: within written, signed, or spoken discourse. Language documentation combines anthropological inquiry with linguistic inquiry to describe languages and their grammars. Lexicography covers the study and construction of dictionaries. Computational linguistics applies computer technology to\n"
     ]
    }
   ],
   "source": [
    "query = \"linguistics\"\n",
    "\n",
    "print(f\"#> {query}\")\n",
    "\n",
    "# Find the top-3 passages for this query\n",
    "results = searcher.search(query, k=3) \n",
    "\n",
    "# Print out the top-k retrieved passages\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    print(f\"\\t[{passage_rank}]\\t{passage_score:.1f}\\t {searcher.collection[passage_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9n0_xwdYre1",
    "tags": []
   },
   "source": [
    "### Retrieval evaluation\n",
    "\n",
    "For more rigorous evaluations of the retriever alone, we can use Sucess@`k` defined relative to the SQuAD passages and answers. We say that we have a \"success\" if a passage in the top `k` retrieved passages contains any of the answers substrings, and Sucess@`k` is the percentage of such success cases. This is very heuristic (perhaps the answer string happens to occur somewhere in a completely irrelevant passage), but it can still be good guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "fsNayHzdYre1"
   },
   "outputs": [],
   "source": [
    "def success_at_k(examples, k=20):\n",
    "    scores = []\n",
    "    for ex in examples: \n",
    "        scores.append(evaluate_retrieval_example(ex, k=k))\n",
    "    return sum(scores) / len(scores)\n",
    "        \n",
    "    \n",
    "def evaluate_retrieval_example(ex, k=20):    \n",
    "    results = searcher.search(ex.question, k=k)\n",
    "    for passage_id, passage_rank, passage_score in zip(*results):\n",
    "        passage = searcher.collection[passage_id]\n",
    "        score = has_answer([DPR_normalize(ans) for ans in ex.answers], passage)\n",
    "        if score:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2U4v58dYre1"
   },
   "source": [
    "Here is Sucess@20 for the SQuAD dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2oEmspeYre1",
    "outputId": "bfff7ec2-26e4-47ef-a101-04fff7fd0621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815\n",
      "CPU times: user 20min 59s, sys: 1min 45s, total: 22min 45s\n",
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    # This will take a few hours on a CPU:\n",
    "    print(success_at_k(squad_dev, k=5))\n",
    "else:\n",
    "    # This should be reasonably fast and yields the\n",
    "    # same kind of result:\n",
    "    print(success_at_k(dev_exs, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teKobQM8Yre1",
    "tags": []
   },
   "source": [
    "## Zero-shot OpenQA with ColBERT retrieval\n",
    "\n",
    "We're now in a position to define a system that does our full few-shot OpenQA task. To get this started, we define just a version that doesn't include any SQuaD-training examples in the prompt. So this is really zero-shot OpenQA. (The homework asks you to move to the true few-shot setting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "jbn4BHr4Yre1"
   },
   "outputs": [],
   "source": [
    "def build_zero_shot_openqa_prompt(question, passage, joiner=\"\\n\\n\"):\n",
    "    title, context = passage.split(\" | \", 1)\n",
    "    segs = [\n",
    "        f\"Title: {title}\",\n",
    "        f\"Background: {context}\",\n",
    "        f\"Q: {question}\",\n",
    "        \"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Xjz7l_yQYre2"
   },
   "outputs": [],
   "source": [
    "def evaluate_zero_shot_openqa(examples, joiner=\"\\n\\n\", gen_func=run_eleuther, batch_size=20):\n",
    "    prompts = []\n",
    "    gens = []\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        exs = examples[i: i+batch_size]\n",
    "        results = [searcher.search(ex.question, k=1) for ex in exs]\n",
    "        passages = [searcher.collection[r[0][0]] for r in results]\n",
    "        ps = [build_zero_shot_openqa_prompt(ex.question, psg, joiner=joiner) \n",
    "              for ex, psg in zip(exs, passages)]\n",
    "        gs = gen_func(ps)       \n",
    "        prompts += ps\n",
    "        gens += gs\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21DfS0hHYre2",
    "outputId": "b653e3e8-252c-47f8-8481-c190aca6c286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05866163979078907\n",
      "CPU times: user 22min 23s, sys: 2min 19s, total: 24min 42s\n",
      "Wall time: 7min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "zero_shot_openqa_results = evaluate_zero_shot_openqa(dev_exs)\n",
    "\n",
    "print(zero_shot_openqa_results['macro_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNhMla71Yre2",
    "outputId": "d6a6d52b-4af5-4519-cd7f-2298e859cc79"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "zero_shot_openqa_results_gpt3 = evaluate_zero_shot_openqa(dev_exs, gen_func=run_gpt3)\n",
    "\n",
    "zero_shot_openqa_results_gpt3['macro_f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "outblNTaYre2",
    "tags": []
   },
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Few-shot OpenQA with no context [2 points]\n",
    "\n",
    "In the section [Open QA with no context](#Open-QA-with-no-context) above, we simply prompted our LM with a question string and looked at what came back. This is arguably unfair to the LM, since we didn't convey anything about our intentions.\n",
    "\n",
    "For a fairer assessment of what the LM alone can do, we should move to the few-shot setting by giving the model a few examples of what we have in mind. The idea here is to create prompts that look like this:\n",
    "\n",
    "   ```   \n",
    "   Q: What is pragmatics?\n",
    "\n",
    "   A: The study of language use\n",
    "\n",
    "   Q: Who is Bert?\n",
    "\n",
    "   A: Bert is one of the Muppets.\n",
    "\n",
    "   Q: What was Stanford University founded?\n",
    "   \n",
    "   A: \n",
    "   ```\n",
    "   \n",
    "This question asks you to write a function for creating such prompts, using SQuAD training examples, and a second function for evaluating this approach. The goal is to have a no context baseline for the other few-shot approaches we are considering.\n",
    "\n",
    "__Task 1___: Complete the function `build_few_shot_no_context_prompt` so that it builds prompts like the above. You can use `test_build_few_shot_no_context_prompt` to check that your function is returning prompts in the desired format.\n",
    "\n",
    "__Task 2__: Complete the function `evaluate_few_shot_no_context` so that you can evaluate this approach. You can use `test_evaluator` to check that your function is performing the desired kind of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_no_context_prompt(question, train_exs, joiner=\"\\n\\n\"):\n",
    "    \"\"\"No context few-shot OpenQA prompts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str   \n",
    "    train_exs : iterable of SQuAD train examples. These can be \n",
    "        obtained via a random sample \n",
    "        from `squad_train` as defined above.\n",
    "    joiner : str\n",
    "        The character to use to join pieces of the prompt into \n",
    "        a single str.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str, the prompt\n",
    "\n",
    "    \"\"\"\n",
    "    segs = []\n",
    "    for t in train_exs:\n",
    "        segs += [\n",
    "            f\"Q: {t.question}\",\n",
    "            f\"A: {t.answers[0]}\"\n",
    "        ]\n",
    "    segs += [\n",
    "        f\"Q: {question}\",\n",
    "        f\"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_build_few_shot_no_context_prompt(func):\n",
    "    train_exs = [\n",
    "        SquadExample(0, \"T1\", \"Q1\", \"C1\", [\"A1\"]),\n",
    "        SquadExample(1, \"T2\", \"Q2\", \"C2\", [\"A2\"]),\n",
    "        SquadExample(2, \"T3\", \"Q3\", \"C3\", [\"A3\"])]\n",
    "    question = \"My Q\"\n",
    "    result = func(question, train_exs, joiner=\"\\n\")\n",
    "    expected = \"\"\n",
    "    tests = [\n",
    "        (1, \"\\n\", 'Q: C1\\nA: A1\\nQ: My Q\\nA:'),                \n",
    "        (1, \"\\n\\n\", 'Q: C1\\n\\nA: A1\\n\\nQ: My Q\\n\\nA:'),\n",
    "        (2, \"\\n\", 'Q: C1\\nA: A1\\nQ: C2\\nA: A2\\nQ: My Q\\nA:')]\n",
    "    err_count = 0       \n",
    "    for n_context, joiner, expected in tests:\n",
    "        result = func(question, train_exs[: n_context], joiner=joiner)\n",
    "        if result != expected:\n",
    "            err_count +=1 \n",
    "            print(f\"Error:\\n\\nExpected:\\n\\n{expected}\\n\\nGot:\\n\\n{result}\")    \n",
    "    if err_count == 0:\n",
    "        print(\"No errors detected in `build_few_shot_no_context_prompt`\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected in `build_few_shot_no_context_prompt`\n"
     ]
    }
   ],
   "source": [
    "test_build_few_shot_no_context_prompt(build_few_shot_no_context_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot_no_context(\n",
    "        examples,\n",
    "        squad_train,\n",
    "        batch_size=20,\n",
    "        n_context=2,\n",
    "        joiner=\"\\n\\n\",\n",
    "        gen_func=run_eleuther):\n",
    "    \"\"\"Evaluate a few-shot OpenQA with no context approach \n",
    "    defined by `build_few_shot_no_context_prompt` and `gen_func`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : iterable of SQuAD train examples\n",
    "        Presumably a subset of `squad_dev` as defined above.\n",
    "    squad_train : iterable of SQuAD train examples\n",
    "    batch_size : int\n",
    "        Number of examples to send to `gen_func` at once.\n",
    "    n_context : n\n",
    "        Number of examples to use from `squad_train`.\n",
    "    joiner : str\n",
    "        Used by `build_few_shot_open_qa_prompt` to join segments\n",
    "        of the prompt into a single str.\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict as determined by `evaluate` above.\n",
    "\n",
    "    \"\"\"\n",
    "    # A list of strings that you build and feed into `gen_func`.\n",
    "    prompts = []\n",
    "\n",
    "    # A list of dicts that you get from `gen_func`.\n",
    "    gens = []\n",
    "\n",
    "    # Iterate through the examples in batches:\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        # Sample some SQuAD training examples to use with\n",
    "        # `build_few_shot_no_context_prompt` and `ex.question`,\n",
    "        # run the resulting prompt through `gen_func`, and\n",
    "        # add your prompts and results to `prompts` and `gens`.\n",
    "        batch = examples[i: i+batch_size]\n",
    "        ps = [build_few_shot_no_context_prompt(ex, \n",
    "                                               random.sample(squad_train, k=n_context)) \n",
    "              for ex in batch]        \n",
    "        gs = gen_func(ps)       \n",
    "        prompts += ps\n",
    "        gens += gs\n",
    "\n",
    "\n",
    "    # Return value from a call to `evalaute`, with `examples`\n",
    "    # as provided by the user and the `prompts` and `gens`\n",
    "    # you built:\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_evaluator(func):\n",
    "    examples = [SquadExample(0, \"T1\", \"Q1\", \"C1\", [\"A1\"])]    \n",
    "    squad_train = [SquadExample(0, \"sT1\", \"sQ1\", \"sC1\", [\"sA1\"])] \n",
    "    \n",
    "    def gen_func(*prompts):\n",
    "        return [{\n",
    "            \"generated_answer\": \"Constant output\", \n",
    "            \"generated_answer_tokens\": [\"Constant\", \"output\"], \n",
    "            \"generated_answer_probs\": [0.1, 0.2]}]\n",
    "    \n",
    "    batch_size = 1    \n",
    "    n_context = 1    \n",
    "    joiner = \"\\n\"\n",
    "    result = func(\n",
    "        examples, \n",
    "        squad_train, \n",
    "        batch_size=1, \n",
    "        n_context=1, \n",
    "        joiner=joiner, \n",
    "        gen_func=gen_func)\n",
    "    expected_keys = {'em_per', 'examples', 'macro_f1'}\n",
    "    result_keys = set(result.keys())     \n",
    "    if expected_keys != result_keys:\n",
    "        print(f\"Unexpected keys in result. \"\n",
    "              f\"Expected: {expected_keys}; Got: {result_keys}\")\n",
    "        return\n",
    "    expected_ex_keys = {\n",
    "        'f1', 'id', 'em', 'generated_answer_tokens', 'generated_answer_probs',\n",
    "        'prediction', 'generated_answer', 'question', 'answers'}\n",
    "    result_ex_keys = set(result[\"examples\"][0].keys())\n",
    "    if expected_ex_keys != result_ex_keys:\n",
    "        print(f\"Unexpected keys in result['examples']. \"\n",
    "              f\"Expected: {expected_ex_keys}; Got: {result_ex_keys}\")\n",
    "        return\n",
    "    print(\"No errors detected in `evaluate_few_shot_open_qa`\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected in `evaluate_few_shot_open_qa`\n"
     ]
    }
   ],
   "source": [
    "test_evaluator(evaluate_few_shot_no_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2mx3Z4HYre2",
    "tags": []
   },
   "source": [
    "### Few-shot OpenQA [2 points]\n",
    "\n",
    "In the section [Few-shot QA](Few-shot-QA) above, we used SQuAD training examples to build prompts that we hope will help the model infer our intended semantics for the prompts themselves. When we moved to the open formulation of the problem, in [Open QA with ColBERT retrieval](Open-QA-with-ColBERT-retrieval), we forced the model to deal with prompts that lack these context clues. This is a \"zero-shot\" formulation of the problem. The goal of this homework problem is to improve that system so that it truly supports few-shot OpenQA.\n",
    "\n",
    "__Task 1__: Complete the function `build_few_shot_open_qa_prompt` so that it builds prompts from a question, a passage, and a sample of SQuAD training examples. You can use `test_build_few_shot_open_qa_prompt` to check that your function is returning prompts in the desired format.\n",
    "\n",
    "__Task 2__: Complete the function `evaluate_few_shot_open_qa` so that you can evaluate this approach. You can use `test_evaluator` from above to check that your function is performing the desired kind of evaluation.\n",
    "\n",
    "We will be checking only that the tests pass. We will not be evaluating the quality of the results you obtain using this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "HUuZ5l3gYre2"
   },
   "outputs": [],
   "source": [
    "def build_few_shot_open_qa_prompt(question, passage, train_exs, joiner=\"\\n\\n\"):\n",
    "    \"\"\"Few-shot OpenQA prompts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "    passage : str\n",
    "        Presumably something retrieved via search.\n",
    "    train_exs : iterable of SQuAD train examples\n",
    "        These can be obtained via a random sample from \n",
    "        `squad_train` as defined above.\n",
    "    joiner : str\n",
    "        The character to use to join pieces of the prompt \n",
    "        into a single str.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str, the prompt\n",
    "\n",
    "    \"\"\"\n",
    "    segs = []\n",
    "    for t in train_exs:\n",
    "        segs += [\n",
    "            f\"Title: {t.title}\",\n",
    "            f\"Background: {t.context}\",\n",
    "            f\"Q: {t.question}\",\n",
    "            f\"A: {t.answers[0]}\",\n",
    "        ]\n",
    "    t, p = passage.split(' | ')\n",
    "    segs += [\n",
    "        f\"Title: {t}\",\n",
    "        f\"Background: {p}\",\n",
    "        f\"Q: {question}\",\n",
    "        f\"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "vgeNwTu4Yre2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_build_few_shot_open_qa_prompt(func):\n",
    "    train_exs = [\n",
    "        SquadExample(0, \"T1\", \"Q1\", \"C1\", [\"A1\"]),\n",
    "        SquadExample(1, \"T2\", \"Q2\", \"C2\", [\"A2\"]),\n",
    "        SquadExample(2, \"T3\", \"Q3\", \"C3\", [\"A3\"])]            \n",
    "    question = \"My Q\"    \n",
    "    passage = \"Title | target passage\"    \n",
    "    tests = [\n",
    "        (1, \"\\n\", ('Title: T1\\nBackground: Q1\\nQ: C1\\nA: A1\\n'\n",
    "                   'Title: Title\\nBackground: target passage\\nQ: My Q\\nA:')),\n",
    "        (1, \"\\n\\n\", ('Title: T1\\n\\nBackground: Q1\\n\\nQ: C1\\n\\nA: A1\\n\\n'\n",
    "                     'Title: Title\\n\\nBackground: target passage\\n\\nQ: My Q\\n\\nA:')),\n",
    "        (2, \"\\n\", ('Title: T1\\nBackground: Q1\\nQ: C1\\nA: A1\\nTitle: T2\\n'\n",
    "                   'Background: Q2\\nQ: C2\\nA: A2\\nTitle: Title\\n'\n",
    "                   'Background: target passage\\nQ: My Q\\nA:'))]\n",
    "    err_count = 0       \n",
    "    for n_context, joiner, expected in tests:\n",
    "        result = func(question, passage, train_exs[: n_context], joiner=joiner)\n",
    "        if result != expected:\n",
    "            err_count +=1 \n",
    "            print(f\"Error:\\n\\nExpected:\\n\\n{expected}\\n\\nGot:\\n\\n{result}\")    \n",
    "    if err_count == 0:\n",
    "        print(\"No errors detected in `build_few_shot_open_qa_prompt`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lK991-2AYre3",
    "outputId": "b8134fd6-d477-41e4-9d70-cbed90f17a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected in `build_few_shot_open_qa_prompt`\n"
     ]
    }
   ],
   "source": [
    "test_build_few_shot_open_qa_prompt(build_few_shot_open_qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "DRhoMeEGYre3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot_open_qa(\n",
    "        examples,\n",
    "        squad_train,\n",
    "        batch_size=20,\n",
    "        n_context=2,\n",
    "        joiner=\"\\n\\n\",\n",
    "        gen_func=run_eleuther):\n",
    "    \"\"\"Evaluate a few-shot OpenQA approach defined by \n",
    "    `build_few_shot_open_qa_prompt` and `gen_func`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : iterable of SQuAD train examples\n",
    "        Presumably a subset of `squad_dev` as defined above.\n",
    "    squad_train : iterable of SQuAD train examples\n",
    "    batch_size : int\n",
    "        Number of examples to send to `gen_func` at once.\n",
    "    joiner : str\n",
    "        Used by `build_few_shot_open_qa_prompt` to join segments\n",
    "        of the prompt into a single str.\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict as determined by `evaluate` above.\n",
    "\n",
    "    \"\"\"\n",
    "    # A list of strings that you build and feed into `gen_func`.\n",
    "    prompts = []\n",
    "\n",
    "    # A list of dicts that you get from `gen_func`.\n",
    "    gens = []\n",
    "\n",
    "    # Iterate through the examples in batches:\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        batch = examples[i: i+batch_size]\n",
    "        ps = [build_few_shot_open_qa_prompt(ex.question, f\"{ex.title} | {ex.context}\", \n",
    "                                            random.sample(squad_train, k=n_context)) \n",
    "              for ex in batch]        \n",
    "        gs = gen_func(ps)       \n",
    "        prompts += ps\n",
    "        gens += gs\n",
    "\n",
    "\n",
    "    # Return value from a call to `evalaute`, with `examples`\n",
    "    # as provided by the user and the `prompts` and `gens`\n",
    "    # you built:\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beEyy_eOYre3",
    "outputId": "22b49855-289f-43e8-f214-5d18238a00b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected in `evaluate_few_shot_open_qa`\n"
     ]
    }
   ],
   "source": [
    "test_evaluator(evaluate_few_shot_open_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXeQzplkYre3",
    "tags": []
   },
   "source": [
    "### Answer scoring [2 points]\n",
    "\n",
    "We have so far been assuming that the top-ranked passage retrieved by ColBERT should be used in the prompt and that the single answer returned by the LM is our prediction. It may be possible to improve on this by scoring answers using the ColBERT scores and the probabilities returned by the LM. This question asks you to explore a basic approach to such scoring. The core scoring function:\n",
    "\n",
    "$$\n",
    "\\textbf{score}_{\\text{prompt-func}}(\\textrm{answer}, \\textrm{passage}, \\textrm{question}) = \n",
    "P(\\textrm{passage} \\mid \\textrm{question}) \\cdot \n",
    "P(\\textrm{answer} \\mid \\text{prompt-func}(\\textrm{question}, \\textrm{passage}) ) \n",
    "$$\n",
    "\n",
    "where we estimate the two conditional probabilities as follows:\n",
    "\n",
    "* $P(\\textrm{passage} \\mid \\textrm{question})$ is defined only for the top $k$ passages and defined by the softmax of the top $k$ scores returned by the retriever.\n",
    "\n",
    "* $P(\\textrm{answer} \\mid \\text{prompt-func}(\\textrm{question}, \\textrm{passage}))$ is simply the product of the per-token probabilities of the generated answer given the prompt determined by $\\text{prompt-func}(\\textrm{question}, \\textrm{passage})$. These values can be extracted from the return values of both `run_eleuther` and `run_gpt3` using the key `\"generated_answer_probs\"`. (Your prompt function might of course have other arguments not represented here.)\n",
    "\n",
    "__Your task__: Implement this scoring function for an individual example. The two required pieces are `get_passages_with_scores` and `answer_scoring`. Starter code for each is below, and each has a unit test you can run to check your work.\n",
    "\n",
    "(With this implemented, it is easy to create a new prediction function that uses the $\\textrm{answer}$ from the highest-scoring $\\textrm{answer}/\\textrm{passage}$ pair as the prediction for input $\\textrm{question}$. You are not required to implement such a prediction function, but you might do this as part of [your original system](#Your-original-system-[3-points]).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "m7PhfMNsYre3"
   },
   "outputs": [],
   "source": [
    "def get_passages_with_scores(question, k=5):\n",
    "    \"\"\"Pseudo-probabilities from the retriever.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "    k : int\n",
    "        Number of passages to retrieve.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    passages (list of str), passage_probs (np.array)\n",
    " \n",
    "    \"\"\"\n",
    "    # Use the `searcher` to get `k` passages for `questions`:\n",
    "    results = searcher.search(question, k=k) \n",
    "\n",
    "    # Softmax normalize the scores and convert the list to\n",
    "    # a NumPy array:\n",
    "    passage_probs = np.exp(results[2])/np.sum(np.exp(results[2]))\n",
    "\n",
    "    # Get the passages as a list of texts:\n",
    "    passages = [searcher.collection[passage_id] for passage_id in results[0]]\n",
    "\n",
    "    return passages, passage_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "-vqCNOtMYre4"
   },
   "outputs": [],
   "source": [
    "def test_get_passages_with_scores(func):\n",
    "    question = \"What is linguistics?\"        \n",
    "    passages, passage_probs = get_passages_with_scores(question, k=2)    \n",
    "    if len(passages) != len(passage_probs):\n",
    "        print(\"`get_passages_with_scores` should return equal length \"\n",
    "              \"lists of passages and passage probabilities.\")\n",
    "        return\n",
    "    if len(passages) != 2:\n",
    "        print(f\"`get_passages_with_scores` should return `k` passages. Yours returns {len(passages)}\")\n",
    "        return\n",
    "    if not all(isinstance(psg, str) for psg in passages):\n",
    "        print(\"The first return argument should be a list of passage strings.\")\n",
    "        return\n",
    "    if not all(isinstance(p, (float, np.float32, np.float64)) for p in passage_probs): \n",
    "        print(\"The second return argument should be a list of floats.\")\n",
    "        return \n",
    "    print(\"No errors detected in `get_passages_with_scores`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfsb4pyHYre4",
    "outputId": "f13fc8e2-f21b-44ec-b16f-caa2855b8136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected in `get_passages_with_scores`\n"
     ]
    }
   ],
   "source": [
    "test_get_passages_with_scores(get_passages_with_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "SwQVdCb6Yre4"
   },
   "outputs": [],
   "source": [
    "def answer_scoring(passages, passage_probs, prompts, gen_func=run_eleuther):\n",
    "    \"\"\"Implements our basic scoring strategy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    passages : list of str\n",
    "    passage_probs : list of float\n",
    "    prompts : list of str\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of pairs (score, dict), sorted with the largest score first.\n",
    "    `dict` should be the return value of `gen_func` for an example.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for passage, passage_prob, prompt in zip(passages, passage_probs, prompts):\n",
    "        # Run `gen_func` on [prompt] (crucially, the singleton list here),\n",
    "        # and get the dictionary `gen` from the singleton list `gen_func`\n",
    "        # returns, and then use the values to score `gen` according to our\n",
    "        # scoring method.\n",
    "        #\n",
    "        # Be sure to use \"generated_answer_probs\" for the scores.\n",
    "        ##### YOUR CODE HERE\n",
    "        gen = gen_func(prompt)\n",
    "        # print(gen)\n",
    "        score = np.prod(gen[0][\"generated_answer_probs\"])\n",
    "        data.append((score, gen[0]))\n",
    "\n",
    "\n",
    "    # Return `data`, sorted with the highest scoring `(score, gen)`\n",
    "    # pair given first.\n",
    "    return sorted(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "JD7d8ucgYre4"
   },
   "outputs": [],
   "source": [
    "def test_answer_scoring(func):\n",
    "    passages = [\n",
    "        \"Pragmatics is the study of language use.\", \n",
    "        \"Phonology is the study of linguistic sound systems.\"]\n",
    "    passage_probs = [0.75, 0.25]\n",
    "    prompts = passages\n",
    "    \n",
    "    def gen_func(*prompts):\n",
    "        return [{\n",
    "            \"generated_answer\": \"Constant output\", \n",
    "            \"generated_answer_tokens\": [\"Constant\", \"output\"], \n",
    "            \"generated_answer_probs\": [0.1, 0.2]}]\n",
    "    \n",
    "    data = func(passages, passage_probs, prompts, gen_func=gen_func)\n",
    "    \n",
    "    if not all(len(x) == 2 for x in data):\n",
    "        print(\"`answer_scoring` should return a list of pairs (score, gen)\")\n",
    "        return \n",
    "    if not isinstance(data[0][0], (float, np.float32, np.float64)):\n",
    "        print(\"The first member of each pair in `data` should be a score (type `float`).\")\n",
    "        return    \n",
    "    if not isinstance(data[0][1], dict):\n",
    "        print(\"The second member of each pair in `data` should be a dict \" \n",
    "              \"created by running `gen_func` on a single example.\")\n",
    "        return    \n",
    "    if data[0][0] != max([x for x, y in data]):\n",
    "        print(\"`answer_scoring` should sort its data with the highest score first.\")\n",
    "        return \n",
    "    \n",
    "    print(\"No errors detected in `answer_scoring`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWwwDzvBYre4",
    "outputId": "f591f713-3572-4f09-a550-08fd1595835f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected in `answer_scoring`\n"
     ]
    }
   ],
   "source": [
    "test_answer_scoring(answer_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "OUcdfU9SYre4"
   },
   "outputs": [],
   "source": [
    "def answer_scoring_demo(question):\n",
    "    \"\"\"Example usage for answer_scoring. Here we extract the top-scoring\n",
    "    results, which can then be used in an evaluation.\"\"\"    \n",
    "    passages, passage_probs = get_passages_with_scores(question)\n",
    "    prompts = [build_zero_shot_openqa_prompt(question, psg) for psg in passages]\n",
    "    data = answer_scoring(passages, passage_probs, prompts)\n",
    "    # Top-scoring answer string:\n",
    "    return data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmURWvoJYre4"
   },
   "source": [
    "### Your original system [3 points]\n",
    "\n",
    "This question asks you to design your own few-shot OpenQA system. All of the code above can be used and modified for this, and the requirement is just that you try something new that goes beyond what we've done so far. \n",
    "\n",
    "Terms for the bake-off:\n",
    "\n",
    "* You can make free use of SQuAD and other publicly available data.\n",
    "* The LM must be an autoregressive language model. No trained QA components can be used. Our list of preallowed models are those available via the OpenAI API whose names begin with \"text\" and the Eluether models \"gpt-neo-125M\", \"gpt-neo-1.3B\", \"gpt-neo-2.7B\", and \"gpt-j-6B\". If you would like to use a model outside of this set, please check with the teaching team first.\n",
    "\n",
    "Here are some ideas for the original system:\n",
    "\n",
    "* We have so far sampled randomly from the SQuaD train set to create few-shot prompts. One might instead sample passages that have some connection to the target question.\n",
    "\n",
    "* We have used actual SQuAD training examples to build contexts. These might be different in meaningful ways from the passages in our corpus. An alternative is to use the SQuAD question–answer pairs to retrieve passages that contain the answer and use the resulting question–answer–passage triple when building prompts.\n",
    "\n",
    "* There are a lot of parameters to our LMs that we have so far ignored. Exploring different values might lead to better results. The `temperature` parameter is highly impactful for our task.\n",
    "\n",
    "* We have distributed a fixed index of 100K passages. These cover SQuAD plus our bake-off data, but there might still be value in creating a different/expanded index. There is starter code for indexing data with ColBERT [here](https://github.com/stanford-futuredata/ColBERT/blob/new_api/docs/intro.ipynb).\n",
    "\n",
    "* [Khattab et al. (2021a)](https://aclanthology.org/2021.tacl-1.55/) fine-tune the retriever through a handful of successive rounds, using weak supervision from the QA dataset. This is an ambitious direction that could quickly build to an original project, as the role of retriever training is under-explored so far in the context of few-shot OpenQA.\n",
    "\n",
    "* In our \"Answer scoring\" question, we don't normalize scores by answer length. Such normalization might be fairer to long answers and so seems worth adding.\n",
    "\n",
    "* Our \"Answer scoring\" question is inspired by the Retrieval Augmented Generation (RAG) model of [Lewis et al. 2020](https://arxiv.org/abs/2005.11401). Their model fully marginalizes over $k$ retrieved passages to create a proper model of $P(\\textrm{answer} \\mid \\textrm{question})$. Implementing this requires having the probabilities for the prompts. For GPT-3, these can be obtained with `echo=False`, which will lead you to have to make changes to the output processing of `run_gpt3`. For the Eleuther models, one needs to do another call to the model forward function. Here is some starter code that could be used to begin modifying `run_eleuther`:\n",
    "\n",
    "   ```\n",
    "    prompt_logits = eleuther_model(prompt_ids).logits                \n",
    "    prompt_probs = prompt_logits.softmax(-1)                                   \n",
    "    prompt_probs = torch.gather(prompt_probs, 2, prompt_ids[:, :, None]).squeeze(-1)\n",
    "    prompt_probs = [list(prompt_prob.numpy()) for p in prompt_probs]\n",
    "   ```\n",
    "\n",
    "__Original system instructions__:\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies. \n",
    "\n",
    "We also ask that you report the best macro F1 score your system got during development on `dev_exs` [as defined above](#SQuAD-dev-sample), just to help us understand how systems performed overall.\n",
    "\n",
    "Please review the descriptions in the following comment and follow the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "id": "D9pfDIrPYre4",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dsp-ml in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: backoff in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (2.2.1)\n",
      "Requirement already satisfied: datasets in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (2.0.0)\n",
      "Requirement already satisfied: openai in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (0.26.5)\n",
      "Requirement already satisfied: ujson in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (5.7.0)\n",
      "Requirement already satisfied: spacy in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (3.5.0)\n",
      "Requirement already satisfied: jupyter in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (1.2.0)\n",
      "Requirement already satisfied: pandas in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (1.5.0)\n",
      "Requirement already satisfied: regex in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from dsp-ml) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (1.23.1)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (10.0.1)\n",
      "Requirement already satisfied: packaging in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (21.3)\n",
      "Requirement already satisfied: aiohttp in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (2023.1.0)\n",
      "Requirement already satisfied: multiprocess in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (0.70.14)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (0.11.1)\n",
      "Requirement already satisfied: dill in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (2.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from datasets->dsp-ml) (0.18.0)\n",
      "Requirement already satisfied: qtconsole in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter->dsp-ml) (5.4.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter->dsp-ml) (8.0.2)\n",
      "Requirement already satisfied: ipykernel in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter->dsp-ml) (6.16.0)\n",
      "Requirement already satisfied: jupyter-console in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter->dsp-ml) (6.5.1)\n",
      "Requirement already satisfied: notebook in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter->dsp-ml) (6.4.12)\n",
      "Requirement already satisfied: nbconvert in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter->dsp-ml) (7.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from pandas->dsp-ml) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from pandas->dsp-ml) (2022.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (62.3.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (8.1.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (1.10.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (2.4.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (1.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (0.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from spacy->dsp-ml) (3.0.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets->dsp-ml) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets->dsp-ml) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets->dsp-ml) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets->dsp-ml) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets->dsp-ml) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets->dsp-ml) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from aiohttp->datasets->dsp-ml) (6.0.4)\n",
      "Requirement already satisfied: filelock in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->dsp-ml) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->dsp-ml) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->dsp-ml) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from packaging->datasets->dsp-ml) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->dsp-ml) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.19.0->datasets->dsp-ml) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.19.0->datasets->dsp-ml) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from requests>=2.19.0->datasets->dsp-ml) (1.26.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy->dsp-ml) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy->dsp-ml) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy->dsp-ml) (8.1.3)\n",
      "Requirement already satisfied: appnope in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (0.1.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (0.1.6)\n",
      "Requirement already satisfied: psutil in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (5.9.2)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (8.5.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (6.2)\n",
      "Requirement already satisfied: nest-asyncio in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (1.5.5)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (5.4.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (1.6.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (24.0.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipykernel->jupyter->dsp-ml) (7.3.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipywidgets->jupyter->dsp-ml) (3.0.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipywidgets->jupyter->dsp-ml) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jinja2->spacy->dsp-ml) (2.1.1)\n",
      "Requirement already satisfied: pygments in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter-console->jupyter->dsp-ml) (2.13.0)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter-console->jupyter->dsp-ml) (3.0.31)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter-console->jupyter->dsp-ml) (5.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (4.12.0)\n",
      "Requirement already satisfied: nbformat>=5.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (5.6.1)\n",
      "Requirement already satisfied: lxml in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (4.9.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (0.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (4.11.1)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (0.6.8)\n",
      "Requirement already satisfied: defusedxml in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (0.7.1)\n",
      "Requirement already satisfied: tinycss2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (1.1.1)\n",
      "Requirement already satisfied: bleach in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbconvert->jupyter->dsp-ml) (5.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from notebook->jupyter->dsp-ml) (0.2.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from notebook->jupyter->dsp-ml) (0.15.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from notebook->jupyter->dsp-ml) (0.14.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from notebook->jupyter->dsp-ml) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from notebook->jupyter->dsp-ml) (21.3.0)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from qtconsole->jupyter->dsp-ml) (2.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from importlib-metadata>=3.6->nbconvert->jupyter->dsp-ml) (3.8.1)\n",
      "Requirement already satisfied: stack-data in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (0.5.1)\n",
      "Requirement already satisfied: backcall in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (0.18.1)\n",
      "Requirement already satisfied: entrypoints in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->dsp-ml) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-console->jupyter->dsp-ml) (3.0.0)\n",
      "Requirement already satisfied: fastjsonschema in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbformat>=5.1->nbconvert->jupyter->dsp-ml) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from nbformat>=5.1->nbconvert->jupyter->dsp-ml) (4.16.0)\n",
      "Requirement already satisfied: wcwidth in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->dsp-ml) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from terminado>=0.8.3->notebook->jupyter->dsp-ml) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from argon2-cffi->notebook->jupyter->dsp-ml) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->jupyter->dsp-ml) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from bleach->nbconvert->jupyter->dsp-ml) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->dsp-ml) (0.18.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->dsp-ml) (1.15.1)\n",
      "Requirement already satisfied: executing in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (1.1.0)\n",
      "Requirement already satisfied: pure-eval in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->dsp-ml) (2.0.8)\n",
      "Requirement already satisfied: pycparser in /Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->dsp-ml) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/Users/lara.thompson/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
    "#   1) Textual description of your system.\n",
    "#   2) The code for your original system.\n",
    "#   3) The score achieved by your system in place of MY_NUMBER.\n",
    "#        With no other changes to that line.\n",
    "#        You should report your score as a decimal value <=1.0\n",
    "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
    "\n",
    "# NOTE: MODULES, CODE AND DATASETS REQUIRED FOR YOUR ORIGINAL SYSTEM\n",
    "# SHOULD BE ADDED BELOW THE 'IS_GRADESCOPE_ENV' CHECK CONDITION. DOING\n",
    "# SO ABOVE THE CHECK MAY CAUSE THE AUTOGRADER TO FAIL.\n",
    "\n",
    "# START COMMENT: Enter your system description in this cell.\n",
    "# I took inspiration from the DSP paper written by our two lecturers (et al) but wanted to\n",
    "# try improving on the prompt cycles. I trimmed down the minnumber of necessary LLM queries to 1\n",
    "# for very short questions, 2 for most questions and 3 for the harder ones (and could have done more,\n",
    "# especially to improve on the dev set! but I got bored of prompt enginering).\n",
    "# My peak score was: 0.372638\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    !pip install dsp-ml\n",
    "    import dsp\n",
    "    import pandas as pd\n",
    "    colbert_server = 'http://ec2-44-228-128-229.us-west-2.compute.amazonaws.com:8893/api/search'\n",
    "    r_search = dsp.ColBERTv2(url=colbert_server)\n",
    "    \n",
    "    \n",
    "    def rm_top_k(query, k=3, source='both'):\n",
    "        tmp = []\n",
    "        if source in ['local', 'both']:\n",
    "            tmp.extend([(p_score, 'local', searcher.collection[p_id]) for p_id, _, p_score in zip(*searcher.search(query, k=k))])\n",
    "        if source in ['remote', 'both']:\n",
    "            tmp.extend([(ans['score'], 'remote', ans['text']) for ans in r_search(query, k=k)])\n",
    "        answers = pd.DataFrame(tmp, columns=['score', 'source', 'text']\n",
    "                              ).drop_duplicates('text').sort_values('score', ascending=False).reset_index(drop=True)\n",
    "        return answers\n",
    "    \n",
    "    initial_query = lambda question: ( \n",
    "        \"Write a search query that will help answer a complex question.\\n\\n\" \n",
    "        \"---\\n\\n\"\n",
    "        \"The format we'll use:\\n\\n\"\n",
    "        \"Question: ${our complex question}\\n\"\n",
    "        \"Rationale: To answer this question step by step, we need to find out ${the missing information}\\n\"\n",
    "        \"Search Query: ${a simple question for seeking the missing information}\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: When was the first man on the moon born?\\n\"\n",
    "        \"Rationale: To answer this question step by step, we need to find out who the first man to walk on the moon was.\\n\"\n",
    "        \"Search Query: first man to walk on the moon\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: The heir to the Du Pont family fortune sponsored what wrestling team?\\n\"\n",
    "        \"Rationale: To answer this question step by step, we need to find out who the heir to the Du Pont family fortune is.\\n\"\n",
    "        \"Search Query: Heir to the Du Pont family fortune\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: What is agoraphobia?\\n\"\n",
    "        \"Rationale: To answer this question step by step, we need to find out what agoraphobia is.\\n\"\n",
    "        \"Search Query: agoraphobia\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: \" + question + '\\n'+\n",
    "        \"Rationale: To answer this question step by step, we need to find out\")\n",
    "    \n",
    "    # some retrieved contexts to use in examples\n",
    "    moon_passage = rm_top_k(\"first man on the moon\", k=2, source='local').loc[1, 'text']\n",
    "    lourde_passage = rm_top_k(\"Who did the Virgin Mary appear to in Lourdes France\", \n",
    "                              k=1, source='local').loc[0, 'text']\n",
    "    rockne_passage = rm_top_k(\"In what year did the team lead by Knute Rockne win the Rose Bowl\", \n",
    "                              k=1, source='local').loc[0, 'text']\n",
    "    dupont_passage = rm_top_k(\"who was heir to the Du Pont family fortune\", \n",
    "                              k=1, source='remote').loc[0, 'text']\n",
    "    dis_passage = rm_top_k(\"What is DIS\", k=1).loc[0, 'text']\n",
    "    \n",
    "    rationale = lambda question, query, context: (\n",
    "        \"Write the step by step analysis of our search results to answer a complex question.\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"The format we'll use:\\n\\n\"\n",
    "        \"Question: ${our complex question}\\n\"\n",
    "        \"Search results for ${the search query}$:\\n\"\n",
    "        \"${what search returned for that query}\\n\"\n",
    "        \"Analysis: To answer this question step by step, we know that ${relevant information from the search results}; we need to know ${the missing information or 'nothing else' if we have all the information we need.}.\\n\"\n",
    "        \"Query: {query for missing information / N/A (if none)}\\n\"\n",
    "        \"Answer: {answer (if we know it) / N/A (if we don't)}\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: When was the first man on the moon born?\\n\"\n",
    "        \"Search results for the first man on the moon:\\n\" + moon_passage + \"\\n\"\n",
    "        \"Analysis: To answer this question step by step, we know that Neil Armstrong was the first man on the moon; we need to know when Neil Armstrong was born.\\n\"\n",
    "        \"Query: Neil Armstrong\\n\"\n",
    "        \"Answer: N/A\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\\n\"\n",
    "        \"Search results for  Who did the Virgin Mary appear to in Lourdes France:\\n\" + lourde_passage + \"\\n\"\n",
    "        \"Analysis: To answer this question step by step, we know that Saint Bernadette Soubirous saw the Virgin Mary in 1958; we need to know nothing else.\\n\"\n",
    "        \"Query: N/A\\n\"\n",
    "        \"Answer: Saint Bernadette Soubirous\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: In what year did the team lead by Knute Rockne win the Rose Bowl?\\n\"\n",
    "        \"Search results for In what year did the team lead by Knute Rockne win the Rose Bowl:\\n\" + rockne_passage + \"\\n\"\n",
    "        \"Analysis: To answer this question step by step, we know that Knute Rockne lead the Notre Dame Team and that the Notre Dame won the Rose Bown in 1925; we need to know nothing else.\\n\"\n",
    "        \"Query: N/A\\n\"\n",
    "        \"Answer: 1925\\n\\n\"\n",
    "        \"---\\n\\n\"             \n",
    "        \"Question: The heir to the Du Pont family fortune sponsored what wrestling team?\\n\"\n",
    "        f\"Search Results:  heir to the Du Pont family fortune\\n{dupont_passage}\\n\"\n",
    "        \"Analysis: To answer this question step by step, we know that John Eleuthère du Pont was heir to the Du Pont family fortune; \"\n",
    "                   \"we need to know what wrestling team John Eleuthère du Pont sponsored.\\n\"\n",
    "        \"Query: wrestling team John Eleuthère du Pont\\n\"\n",
    "        \"Answer: N/A\\n\\n\"\n",
    "        \"---\\n\\n\"             \n",
    "        \"Question: What is DIS?\\n\"\n",
    "        \"Search Results: What is DIS:\\n\" + dis_passage + \"\\n\"\n",
    "        \"Analysis: Developmental Intervention Science (DIS) is a fusion of the literature of both developmental and intervention sciences; we need to know nothing else.\\n\"\n",
    "        \"Query: N/A\\n\"\n",
    "        \"Answer: a fusion of the literature of both developmental and intervention sciences\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Search results for {query}:\\n\" + '\\n'.join(context) + \"\\n\"\n",
    "        \"Analysis: To answer this question step by step, we know that\"\n",
    "    )\n",
    "    \n",
    "    finalize = lambda question, rationale: (\n",
    "        \"Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\\n\"\n",
    "        \"Analysis: we know that Bernadette Soubirous saw the Virgin Mary in 1958\\n\"\n",
    "        \"Answer: Bernadette Soubirous\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: In what year did the team lead by Knute Rockne win the Rose Bowl?\\n\"\n",
    "        \"Analysis: we know that Knute Rockne lead the Notre Dame Team and that the Notre Dame won the Rose Bown in 1925\\n\"\n",
    "        \"Answer: 1925\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: When was the first man on the moon born?\\n\"\n",
    "        \"Analysis: we know that first man on the moon was Neil Armstrong and that he was born August 5, 1930\\n\"\n",
    "        \"Answer: August 5, 1930\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        \"Question: The heir to the Du Pont family fortune sponsored what wrestling team?\\n\"\n",
    "        \"Analysis: we know that John Eleuthère du Pont was an heir to the Du Pont family fortune and that he sponsored Team Foxcatcher\\n\"\n",
    "        \"Answer: Team Foxcatcher\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Analysis: {rationale}\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "    \n",
    "    def e2e(question, engine='text-davinci-003'):\n",
    "        lead_words = ['where', 'what', 'who', 'when', 'that', 'which', 'how',\n",
    "                      'are', 'is', 'was', 'does', 'were', 'did', 'a', 'the']\n",
    "        d = {}\n",
    "        if len(question.split(' ')) < 5: # easy to guess first query\n",
    "            tmp = question\n",
    "            for w in lead_words:\n",
    "                tmp = tmp.removeprefix(w + ' ')\n",
    "            d['query_0'] = tmp.removesuffix('?')\n",
    "        else:\n",
    "            d['result_0'] = run_gpt3([initial_query(question)], engine=engine)[0]['generated_text']\n",
    "            d['query_0'] = d['result_0'].split('Search Query: ')[-1].replace('\\\"','').replace('\\'','')\n",
    "        d['context_0'] = rm_top_k(d['query_0']).loc[:1, 'text'].to_list()\n",
    "        d['result_1'] = run_gpt3([rationale(question, d['query_0'], d['context_0'])], \n",
    "                                 engine=engine)[0]['generated_text']\n",
    "        d['analysis_0'], tmp = d['result_1'].split('; we need to know ')\n",
    "        _, tmp = tmp.split('\\nQuery: ')\n",
    "        d['query_1'],  d['generated_answer'] = tmp.split('\\nAnswer: ')\n",
    "\n",
    "        if d['query_1'] != \"N/A\": # often this works instead of another loop\n",
    "            d['generated_answer'] = run_gpt3([finalize(question, d['analysis_0'])], \n",
    "                                             engine=engine)[0]['generated_text']\n",
    "        return [d['generated_answer']]\n",
    "# STOP COMMENT: Please do not remove this comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS3G0Ss7Yre4"
   },
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, you simply need to be able to run your system on the file \n",
    "\n",
    "```data/openqa/cs224u-openqa-test-unlabeled.txt```\n",
    "\n",
    "The following code should download it for you if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(\"data\", \"openqa\", \"cs224u-openqa-test-unlabeled.txt\")):\n",
    "    !mkdir -p data/openqa\n",
    "    !wget https://web.stanford.edu/class/cs224u/data/cs224u-openqa-test-unlabeled.txt -P data/openqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS3G0Ss7Yre4"
   },
   "source": [
    "If the above fails, you can just download https://web.stanford.edu/class/cs224u/data/cs224u-openqa-test-unlabeled.txt and place it in `data/openqa`.\n",
    "\n",
    "This file contains only questions. The starter code below will help you structure this. It writes a file \"cs224u-openqa-bakeoff-entry.json\" to the current directory. That file should be uploaded as-is. Please do not change its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "GyttYJxoYre4"
   },
   "outputs": [],
   "source": [
    "def create_bakeoff_submission():\n",
    "    filename = os.path.join(\"data\", \"openqa\", \"cs224u-openqa-test-unlabeled.txt\")\n",
    "    \n",
    "    # This should become a mapping from questions (str) to response\n",
    "    # dicts from your system.\n",
    "    gens = {} \n",
    "        \n",
    "    with open(filename) as f:\n",
    "        questions = f.read().splitlines()\n",
    "    \n",
    "    # `questions` is the list of questions you need to evaluate your system on.\n",
    "    # Put whatever code you need to in here to evaluate your system.\n",
    "    # All you need to be sure to do is create a list of dicts with at least\n",
    "    # the keys of the dicts returned by `run_gpt` and `run_eleuther`.\n",
    "    # Add those dicts to `gens`.\n",
    "    #\n",
    "    # Here is an example where we just do \"Open QA with no context\",\n",
    "    # for an \"original system\" that would not earn any credit (since\n",
    "    # it is not original!):\n",
    "    for question in questions:\n",
    "        gens[question] = e2e([question])[0]\n",
    "        \n",
    "    # Quick tests we advise you to run: \n",
    "    # 1. Make sure `gens` is a dict with the questions as the keys:\n",
    "    assert all(q in gens for q in questions)\n",
    "    # 2. Make sure the values are dicts and have the key we will use:\n",
    "    assert all(isinstance(d, dict) and \"generated_answer\" in d for d in gens.values())\n",
    "            \n",
    "    # And finally the output file:\n",
    "    with open(\"cs224u-openqa-bakeoff-entry.json\", \"wt\") as f:\n",
    "        json.dump(gens, f, indent=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [89], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m create_bakeoff_submission()\n",
      "Cell \u001B[0;32mIn [88], line 21\u001B[0m, in \u001B[0;36mcreate_bakeoff_submission\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# `questions` is the list of questions you need to evaluate your system on.\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Put whatever code you need to in here to evaluate your system.\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# All you need to be sure to do is create a list of dicts with at least\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# for an \"original system\" that would not earn any credit (since\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# it is not original!):\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m question \u001B[38;5;129;01min\u001B[39;00m questions:\n\u001B[0;32m---> 21\u001B[0m     gens[question] \u001B[38;5;241m=\u001B[39m \u001B[43mrun_eleuther\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Quick tests we advise you to run: \u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# 1. Make sure `gens` is a dict with the questions as the keys:\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(q \u001B[38;5;129;01min\u001B[39;00m gens \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m questions)\n",
      "Cell \u001B[0;32mIn [85], line 35\u001B[0m, in \u001B[0;36mrun_eleuther\u001B[0;34m(prompts, temperature, top_p, **generate_kwargs)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# Automatic mixed precision if possible.\u001B[39;00m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mamp\u001B[38;5;241m.\u001B[39mautocast() \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m nullcontext():\n\u001B[0;32m---> 35\u001B[0m         model_output \u001B[38;5;241m=\u001B[39m \u001B[43meleuther_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprompt_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m           \u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                \u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meleuther_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Converting output scores using the helpful recipe here:\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\u001B[39;00m\n\u001B[1;32m     49\u001B[0m gen_ids \u001B[38;5;241m=\u001B[39m model_output\u001B[38;5;241m.\u001B[39msequences[:, prompt_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] :]\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m():\n\u001B[0;32m---> 28\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/transformers/generation_utils.py:1217\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1209\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   1210\u001B[0m         input_ids,\n\u001B[1;32m   1211\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mnum_return_sequences,\n\u001B[1;32m   1212\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   1213\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1214\u001B[0m     )\n\u001B[1;32m   1216\u001B[0m     \u001B[38;5;66;03m# 12. run sample\u001B[39;00m\n\u001B[0;32m-> 1217\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1218\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1220\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_warper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1221\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1222\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1223\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1224\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1225\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1226\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1227\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1228\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_beam_gen_mode:\n\u001B[1;32m   1231\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_return_sequences \u001B[38;5;241m>\u001B[39m num_beams:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/transformers/generation_utils.py:1771\u001B[0m, in \u001B[0;36mGenerationMixin.sample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1768\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m   1770\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 1771\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1772\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1773\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1774\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1775\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1776\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[1;32m   1779\u001B[0m     cur_len \u001B[38;5;241m=\u001B[39m cur_len \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:743\u001B[0m, in \u001B[0;36mGPTNeoForCausalLM.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    735\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    736\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m    737\u001B[0m \u001B[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001B[39;00m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001B[39;00m\n\u001B[1;32m    739\u001B[0m \u001B[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[1;32m    740\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    741\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m--> 743\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    744\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    745\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    747\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    749\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    750\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    751\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    752\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    753\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    754\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    755\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    756\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    758\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:622\u001B[0m, in \u001B[0;36mGPTNeoModel.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    614\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    615\u001B[0m         create_custom_forward(block),\n\u001B[1;32m    616\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    619\u001B[0m         head_mask[i],\n\u001B[1;32m    620\u001B[0m     )\n\u001B[1;32m    621\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 622\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    623\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    624\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    625\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:343\u001B[0m, in \u001B[0;36mGPTNeoBlock.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    341\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    342\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_2(hidden_states)\n\u001B[0;32m--> 343\u001B[0m feed_forward_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# residual connection\u001B[39;00m\n\u001B[1;32m    345\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m feed_forward_hidden_states\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:300\u001B[0m, in \u001B[0;36mGPTNeoMLP.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states):\n\u001B[0;32m--> 300\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_fc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    301\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(hidden_states)\n\u001B[1;32m    302\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mc_proj(hidden_states)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/lara.thompson-C83ZgnRu/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001B[0m, in \u001B[0;36mlinear\u001B[0;34m(input, weight, bias)\u001B[0m\n\u001B[1;32m   1846\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[1;32m   1847\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(linear, (\u001B[38;5;28minput\u001B[39m, weight, bias), \u001B[38;5;28minput\u001B[39m, weight, bias\u001B[38;5;241m=\u001B[39mbias)\n\u001B[0;32m-> 1848\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "create_bakeoff_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    " \n",
    "Review and follow the [Homework and bake-off code: Formatting guide](hw_formatting_guide.ipynb).\n",
    "Please **do not change the filename** as described below.\n",
    " \n",
    "Submit the following files to Gradescope:\n",
    " \n",
    "- `hw_openqa.ipynb` (this notebook)\n",
    "- `cs224u-openqa-bakeoff-entry.json` (bake-off output)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hw_openqa_solved.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "a99ac6d2deb03d0b7ced3594556c328848678d7cea021ae1b9990e15d3ad5c49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03b85fa378584208a26436b2c65c62a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "03d8affdbcfb44c08ccdf60e888d9978": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06363edc1de54c15a0a00e104a08d07b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_108c1fbcc56443ba96d57133cad11645",
       "IPY_MODEL_eb96d7ee6af840b2af48783ddad1ddb5",
       "IPY_MODEL_f1726b2d78b54c4aa411b7e4eecbf4eb"
      ],
      "layout": "IPY_MODEL_279a7470d04e4a34b1afdc7d4761c815"
     }
    },
    "06c3d71451a44147892cf77cbee7704e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4047dc7f8fe40e3858efacfe3fa9db3",
       "IPY_MODEL_36a8a57e71b54bf297e8dda3043ff966",
       "IPY_MODEL_78f4d0554a3748328f99d69a22eaa482"
      ],
      "layout": "IPY_MODEL_90cb7850489648eab6264dd813719a9c"
     }
    },
    "0ac869bb35124b39be5bdad7fc78473f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acfc9aa4af0b4ba2a397789fdc63d4cb",
      "max": 1054280,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_254c594f27134d9b9e18736b7f8d0d57",
      "value": 1054280
     }
    },
    "0e16502571d94ea286c1fd32d387e5ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "108c1fbcc56443ba96d57133cad11645": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47a7719397c249389d3ebc5753ba2acf",
      "placeholder": "​",
      "style": "IPY_MODEL_da6c76d121fa4e3ca3d85bc0dcde6212",
      "value": "Downloading: 100%"
     }
    },
    "121976cff01349459b2909033f05c576": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8f1b2d780d74e4382e14974537b1e0b",
       "IPY_MODEL_7211cffdbc0a4ae1a9f894a98cc65b03",
       "IPY_MODEL_577af7ffedd8430da0870d4a9f773056"
      ],
      "layout": "IPY_MODEL_5616a0f10f344b499c9b9e2f2d4f97b9"
     }
    },
    "133bffae565a49e79a395b08f880b3db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "142b1dfda9ae4444a1d577d1c66c215a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1625a303748840aea84cd884c8453578": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19d0a74b2d134747880402c4e6f0fad0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fcb0f73a2e314a8e935f0802f2f12f31",
       "IPY_MODEL_9905ea5f04f34c5badc797bd6fa07d3a",
       "IPY_MODEL_4a08f6612d9d4a3e98d00926e56415b0"
      ],
      "layout": "IPY_MODEL_8a1a0c82837e45709424db63bc86ec86"
     }
    },
    "1a8214cc9451415483ce785b997032e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e16502571d94ea286c1fd32d387e5ea",
      "max": 357,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a093e8d14d6645daae23a16ecd01e423",
      "value": 357
     }
    },
    "1acc184682164c08aee48278ae399814": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b98baadc95644f5b83cc6bfc7cd8c09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23bb67fe91d644469d5be8b442b2e3b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "254c594f27134d9b9e18736b7f8d0d57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "264be22c995f407ab22fb4605601f7c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "279a7470d04e4a34b1afdc7d4761c815": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "280ab82e1c5d4dd8abc4bd15e58034af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87991a91694f456f9318dc5afccec2c9",
      "placeholder": "​",
      "style": "IPY_MODEL_32e27633a7054f17af4dca4ed5e43279",
      "value": "Downloading builder script: "
     }
    },
    "2974fef15c3841baba7035eb6d360156": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c05c539d546447a5882191f1f1e676a9",
      "placeholder": "​",
      "style": "IPY_MODEL_86a5ca66daa64c4b888c375b0d014099",
      "value": "Downloading data: "
     }
    },
    "2badb7d424fa46f6b88b3e7acaf6ab48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b55afcc50d44f5bbdc3b2c6d06bbc87",
      "max": 10570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d84e03f9117448ef970b930aa77fc968",
      "value": 10570
     }
    },
    "2d637c205b844d189f5e4232cb1ffda2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e25e5700521472e83e6888067336682": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fe2c9cf13de4b968b4f318b221aa7dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "304a8303bd0044e6aa53335b6ab30d5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "32e27633a7054f17af4dca4ed5e43279": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32fbedf574754ccc85c4b59d83f743f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c33f1097177468aa401020bac71a5be",
      "placeholder": "​",
      "style": "IPY_MODEL_cef06e938024471cabfcfdad4411b961",
      "value": "Downloading metadata: "
     }
    },
    "3538e755160e478f8cf1262e14f2c484": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b096d4bd8d4497c98176ccf9fe26a13",
       "IPY_MODEL_e7b24e1fa266446cb6bb61ec067fa412",
       "IPY_MODEL_3eccf8ba2e3f4ee4b3a90a1cd89e43b1"
      ],
      "layout": "IPY_MODEL_6af64542f9d547afb656f14a87e6ba03"
     }
    },
    "35e94aa9d357405780025da79e3691af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "368e5fc79b9e4ff9b4fe2e512947d677": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "36a30e94d4dd426db17841370f46faf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36a8a57e71b54bf297e8dda3043ff966": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2fe2c9cf13de4b968b4f318b221aa7dc",
      "max": 87599,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88c02342d65b4b8f9ee1cd25e4ea15ac",
      "value": 87599
     }
    },
    "38030650fe094db6b6436fdcf458a97e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38a2caefa5cd4722be6f553a1d9f7e8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "38cc26500d7f45909637c46e8a676cb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7018804c8581472298d7c8f24beda197",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62196b4464bc48a59fa20f1f28f0fe7b",
      "value": 2
     }
    },
    "3b55afcc50d44f5bbdc3b2c6d06bbc87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3eccf8ba2e3f4ee4b3a90a1cd89e43b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d958f867d5eb40379753a14982302d08",
      "placeholder": "​",
      "style": "IPY_MODEL_76b9355cce89436892b959f00b1cf702",
      "value": " 2/2 [00:00&lt;00:00, 49.35it/s]"
     }
    },
    "3fb7649992084382b327cb61976124ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40c60aa79e644772944b4ffdfd6f7c5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40db9d36656c46238c5ed87adf8cb4ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41f19619d7e347569818bf786673c468": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84da01f23908412b9daef0a8cbcf6337",
      "placeholder": "​",
      "style": "IPY_MODEL_7efeb5f13634479391cffa9a6f9bea49",
      "value": " 2/2 [00:00&lt;00:00, 56.50it/s]"
     }
    },
    "42b97283bd4d4cdc92fed717c2513919": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42c3232aaad049ff92e95957128e2f2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_467d2cd91db246ffaba3af644a47ff1d",
      "placeholder": "​",
      "style": "IPY_MODEL_cb5b0f438ff7492e88d9fe4cc9184e9d",
      "value": " 10093/10570 [00:01&lt;00:00, 8309.99 examples/s]"
     }
    },
    "439bd70a6fda44b993051827f6f072e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_830d66f3b3e64b6f8c86ba775539672e",
      "max": 1021,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d370b83b677d454ea1c81470671443fd",
      "value": 1021
     }
    },
    "446124444abf4c739ad28e593f61562b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "467d2cd91db246ffaba3af644a47ff1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4742b73a39d2444d882aad5b47076911": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47a7719397c249389d3ebc5753ba2acf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47e4b314bc6c435293cb2e5707d49ae1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f208df7109cd475da4f6979e387af2fc",
       "IPY_MODEL_c94c4ffce4cf4f9c93546bbe106bf807",
       "IPY_MODEL_6ceb7c5f67e24c1490ff9d209b2e23dd"
      ],
      "layout": "IPY_MODEL_36a30e94d4dd426db17841370f46faf1"
     }
    },
    "483121c04c514094ba89858a569144d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_872202218d51454faa1211c35a96dc40",
       "IPY_MODEL_b58f664dd2644d43946aec35857d2b11",
       "IPY_MODEL_41f19619d7e347569818bf786673c468"
      ],
      "layout": "IPY_MODEL_3fb7649992084382b327cb61976124ea"
     }
    },
    "4a08f6612d9d4a3e98d00926e56415b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7583f3ce599641d08a625337921e017f",
      "placeholder": "​",
      "style": "IPY_MODEL_e98a1ee4792240b9b7285556d2f3ecbd",
      "value": " 878k/878k [00:00&lt;00:00, 917kB/s]"
     }
    },
    "4b007c951e474e3db19aa16ca96c16a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5071e184457e42bf91a84cc5ea827c19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52cd2dfb427b49feb9546ae276438cc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40c60aa79e644772944b4ffdfd6f7c5a",
      "max": 8116577,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9fd4ae98811341e285ccf84fc7bb73bb",
      "value": 8116577
     }
    },
    "5616a0f10f344b499c9b9e2f2d4f97b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "577af7ffedd8430da0870d4a9f773056": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23bb67fe91d644469d5be8b442b2e3b6",
      "placeholder": "​",
      "style": "IPY_MODEL_264be22c995f407ab22fb4605601f7c3",
      "value": " 502M/502M [00:10&lt;00:00, 47.7MB/s]"
     }
    },
    "5969f13e33594392ad1a9245313fd91f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c12675e10044c6db66daf30f4d4f948": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6510644f7d0545e183053878c6d6c161",
      "placeholder": "​",
      "style": "IPY_MODEL_73cdc7a035d544618e66ed586d7b9649",
      "value": "Downloading data files: 100%"
     }
    },
    "5c33f1097177468aa401020bac71a5be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d67fd950c8e4732a32250c3decb9aef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f72f99cd4094dc292146e35a4d67767": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72570da75be34d37b32be2dae93222ec",
      "placeholder": "​",
      "style": "IPY_MODEL_91859e90f7784956bdc1d40a984d2c5b",
      "value": " 5.27k/? [00:00&lt;00:00, 137kB/s]"
     }
    },
    "62196b4464bc48a59fa20f1f28f0fe7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "62639c9b27be44ee83f9b054f76f651b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6510644f7d0545e183053878c6d6c161": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6af64542f9d547afb656f14a87e6ba03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b096d4bd8d4497c98176ccf9fe26a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62639c9b27be44ee83f9b054f76f651b",
      "placeholder": "​",
      "style": "IPY_MODEL_42b97283bd4d4cdc92fed717c2513919",
      "value": "100%"
     }
    },
    "6b11d0d12a3c42f88defea028bc27545": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b962355e3eb47678013cfd469b57237": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bc69367538f4af683b8a21b3dc59268": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c28e93ddbc77422484ef2de085794312",
      "placeholder": "​",
      "style": "IPY_MODEL_b55b8143e71c4a2cbca7d399fd179797",
      "value": " 2/2 [00:01&lt;00:00,  1.47it/s]"
     }
    },
    "6c7c0e3d3a844558b12dcdb952e76920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ceb7c5f67e24c1490ff9d209b2e23dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_446124444abf4c739ad28e593f61562b",
      "placeholder": "​",
      "style": "IPY_MODEL_d5a9ed85f6424d329727e3651094d394",
      "value": " 446k/446k [00:00&lt;00:00, 3.91MB/s]"
     }
    },
    "7018804c8581472298d7c8f24beda197": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7086129269a04514aa49001d7e712216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7211cffdbc0a4ae1a9f894a98cc65b03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0d37a2cda934241b195138780fbe067",
      "max": 526017373,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9ef856a58be54b9fa8b5c610051c25cf",
      "value": 526017373
     }
    },
    "72570da75be34d37b32be2dae93222ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73b8aa0fe175415a9864d017c5f88682": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73cdc7a035d544618e66ed586d7b9649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7583f3ce599641d08a625337921e017f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76ab7a53d0b64bf8a670dfe31bf72422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38030650fe094db6b6436fdcf458a97e",
      "max": 560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b98baadc95644f5b83cc6bfc7cd8c09",
      "value": 560
     }
    },
    "76b9355cce89436892b959f00b1cf702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78f4d0554a3748328f99d69a22eaa482": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ae88195643b40bda346711154a9c7bd",
      "placeholder": "​",
      "style": "IPY_MODEL_1625a303748840aea84cd884c8453578",
      "value": " 86922/87599 [00:09&lt;00:00, 11041.45 examples/s]"
     }
    },
    "7946163e5da84225a8eccd89122b14df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da906492fc8044dfb04a43afa8a7bbac",
       "IPY_MODEL_76ab7a53d0b64bf8a670dfe31bf72422",
       "IPY_MODEL_c0ab47e3313549cd806f5c10d791d291"
      ],
      "layout": "IPY_MODEL_6b962355e3eb47678013cfd469b57237"
     }
    },
    "7efc61ef150b4287841c06cc291722c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7efeb5f13634479391cffa9a6f9bea49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "830d66f3b3e64b6f8c86ba775539672e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83693e21aeae47b0a5b0ee42eb72a3b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84da01f23908412b9daef0a8cbcf6337": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8639b41657e04b70ac4f3f3ca517a035": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86a5ca66daa64c4b888c375b0d014099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "872202218d51454faa1211c35a96dc40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce46384c19e84fd99447ede08bbaf243",
      "placeholder": "​",
      "style": "IPY_MODEL_03b85fa378584208a26436b2c65c62a8",
      "value": "Extracting data files: 100%"
     }
    },
    "87991a91694f456f9318dc5afccec2c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88c02342d65b4b8f9ee1cd25e4ea15ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88fe4f76341e461abf81f2bf7596913b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5c12675e10044c6db66daf30f4d4f948",
       "IPY_MODEL_38cc26500d7f45909637c46e8a676cb4",
       "IPY_MODEL_6bc69367538f4af683b8a21b3dc59268"
      ],
      "layout": "IPY_MODEL_2d637c205b844d189f5e4232cb1ffda2"
     }
    },
    "8a1a0c82837e45709424db63bc86ec86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ae88195643b40bda346711154a9c7bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c52264dbf2f4cca86e05c112423b536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d14af2ecb8664d26b0792eb47791741b",
       "IPY_MODEL_52cd2dfb427b49feb9546ae276438cc4",
       "IPY_MODEL_f007de44513a41bf95f9f8b861cdaa6f"
      ],
      "layout": "IPY_MODEL_f0d1f5455d934205b20d0b635b746f5d"
     }
    },
    "8c59fe3638c344748d6eed8206e8975d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1acc184682164c08aee48278ae399814",
      "max": 1967,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_368e5fc79b9e4ff9b4fe2e512947d677",
      "value": 1967
     }
    },
    "90cb7850489648eab6264dd813719a9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91859e90f7784956bdc1d40a984d2c5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97c7cc3aa1cb438facac717b44861c71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9905ea5f04f34c5badc797bd6fa07d3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4742b73a39d2444d882aad5b47076911",
      "max": 898669,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9dfca6ee005a4222aa369280c4381a7b",
      "value": 898669
     }
    },
    "9c27bd5813724a2c835e62854b76c2b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c5dd297ecb9495aa82ca68bbec66e1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9dfca6ee005a4222aa369280c4381a7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9ef856a58be54b9fa8b5c610051c25cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9fd4ae98811341e285ccf84fc7bb73bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a093e8d14d6645daae23a16ecd01e423": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a5246564505c48ee98d7644deed5a255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a551f75352ac45b3845c61bbd5c3bb3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83693e21aeae47b0a5b0ee42eb72a3b9",
      "placeholder": "​",
      "style": "IPY_MODEL_03d8affdbcfb44c08ccdf60e888d9978",
      "value": "Downloading: 100%"
     }
    },
    "a7e04c08f551443481414a0dd619006e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3a64df8553f4deea779b13a44233546",
      "placeholder": "​",
      "style": "IPY_MODEL_ad4e6f09e7b342a59235965668fc34a6",
      "value": " 4.85M/? [00:00&lt;00:00, 41.3MB/s]"
     }
    },
    "a8f1b2d780d74e4382e14974537b1e0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdb7e47723fe45e6845a02ad85871d44",
      "placeholder": "​",
      "style": "IPY_MODEL_5d67fd950c8e4732a32250c3decb9aef",
      "value": "Downloading: 100%"
     }
    },
    "ab24a9d6b7a84981ac9d335ff571f7e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac576998bfc940e7af4490bae2ad934f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acfc9aa4af0b4ba2a397789fdc63d4cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad4e6f09e7b342a59235965668fc34a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae5e15d70bf64cde9d8f2caefa2c27f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed33408a0cad4a46a8366263bdf81bfa",
      "placeholder": "​",
      "style": "IPY_MODEL_5969f13e33594392ad1a9245313fd91f",
      "value": "Generating validation split:  95%"
     }
    },
    "b3990b506419477b824530e11f8a42ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b40610e2ebb74029bddea0c818532604": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b55b8143e71c4a2cbca7d399fd179797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b58f664dd2644d43946aec35857d2b11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c760fb9137f44be789276235d2ec552b",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc4900955f81453f98a10ea9b74b983c",
      "value": 2
     }
    },
    "b704aad9cc6e474f988e8966ca179da1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8639b41657e04b70ac4f3f3ca517a035",
      "placeholder": "​",
      "style": "IPY_MODEL_5071e184457e42bf91a84cc5ea827c19",
      "value": " 2.36k/? [00:00&lt;00:00, 74.5kB/s]"
     }
    },
    "bb922ea5e97a44d8b0e94d9f76773167": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_32fbedf574754ccc85c4b59d83f743f4",
       "IPY_MODEL_439bd70a6fda44b993051827f6f072e3",
       "IPY_MODEL_b704aad9cc6e474f988e8966ca179da1"
      ],
      "layout": "IPY_MODEL_dcc8bd4d5b7242ee8c47586cb73c7c0a"
     }
    },
    "c05c539d546447a5882191f1f1e676a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0ab47e3313549cd806f5c10d791d291": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7efc61ef150b4287841c06cc291722c7",
      "placeholder": "​",
      "style": "IPY_MODEL_38a2caefa5cd4722be6f553a1d9f7e8e",
      "value": " 560/560 [00:00&lt;00:00, 17.6kB/s]"
     }
    },
    "c0f59c1508c64a02bdbff82ed569b800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c28e93ddbc77422484ef2de085794312": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3a64df8553f4deea779b13a44233546": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c760fb9137f44be789276235d2ec552b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c83a2859c1e941bca4849872a56baa53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c94c4ffce4cf4f9c93546bbe106bf807": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c83a2859c1e941bca4849872a56baa53",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3990b506419477b824530e11f8a42ef",
      "value": 456318
     }
    },
    "c99ffa80225b4a09ad4e7570b6c4e672": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb5b0f438ff7492e88d9fe4cc9184e9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cdb7e47723fe45e6845a02ad85871d44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce46384c19e84fd99447ede08bbaf243": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cef06e938024471cabfcfdad4411b961": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d14af2ecb8664d26b0792eb47791741b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac576998bfc940e7af4490bae2ad934f",
      "placeholder": "​",
      "style": "IPY_MODEL_4b007c951e474e3db19aa16ca96c16a6",
      "value": "Downloading data: "
     }
    },
    "d370b83b677d454ea1c81470671443fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d556c1920c6f493996a7ba6b0d5fb946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2974fef15c3841baba7035eb6d360156",
       "IPY_MODEL_0ac869bb35124b39be5bdad7fc78473f",
       "IPY_MODEL_a7e04c08f551443481414a0dd619006e"
      ],
      "layout": "IPY_MODEL_b40610e2ebb74029bddea0c818532604"
     }
    },
    "d5a9ed85f6424d329727e3651094d394": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d833a94f93074a4085fb2de3583037f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae5e15d70bf64cde9d8f2caefa2c27f2",
       "IPY_MODEL_2badb7d424fa46f6b88b3e7acaf6ab48",
       "IPY_MODEL_42c3232aaad049ff92e95957128e2f2d"
      ],
      "layout": "IPY_MODEL_9c27bd5813724a2c835e62854b76c2b8"
     }
    },
    "d84e03f9117448ef970b930aa77fc968": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d958f867d5eb40379753a14982302d08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da6c76d121fa4e3ca3d85bc0dcde6212": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da906492fc8044dfb04a43afa8a7bbac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9c217ef31284d95814342521904b45f",
      "placeholder": "​",
      "style": "IPY_MODEL_133bffae565a49e79a395b08f880b3db",
      "value": "Downloading: 100%"
     }
    },
    "dc4900955f81453f98a10ea9b74b983c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dcc8bd4d5b7242ee8c47586cb73c7c0a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0d37a2cda934241b195138780fbe067": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e131d608e26f44f89e1b155fa8042df9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b11d0d12a3c42f88defea028bc27545",
      "placeholder": "​",
      "style": "IPY_MODEL_a5246564505c48ee98d7644deed5a255",
      "value": " 357/357 [00:00&lt;00:00, 10.9kB/s]"
     }
    },
    "e7b24e1fa266446cb6bb61ec067fa412": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f86f33b86afb468c8758174771d676c4",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_304a8303bd0044e6aa53335b6ab30d5a",
      "value": 2
     }
    },
    "e927e8b18836481cb41c7117c0dcf3c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e98a1ee4792240b9b7285556d2f3ecbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb96d7ee6af840b2af48783ddad1ddb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40db9d36656c46238c5ed87adf8cb4ec",
      "max": 1007,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e927e8b18836481cb41c7117c0dcf3c4",
      "value": 1007
     }
    },
    "ed33408a0cad4a46a8366263bdf81bfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f007de44513a41bf95f9f8b861cdaa6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73b8aa0fe175415a9864d017c5f88682",
      "placeholder": "​",
      "style": "IPY_MODEL_c0f59c1508c64a02bdbff82ed569b800",
      "value": " 30.3M/? [00:00&lt;00:00, 73.2MB/s]"
     }
    },
    "f0d1f5455d934205b20d0b635b746f5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1726b2d78b54c4aa411b7e4eecbf4eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c99ffa80225b4a09ad4e7570b6c4e672",
      "placeholder": "​",
      "style": "IPY_MODEL_6c7c0e3d3a844558b12dcdb952e76920",
      "value": " 0.98k/0.98k [00:00&lt;00:00, 30.9kB/s]"
     }
    },
    "f208df7109cd475da4f6979e387af2fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c5dd297ecb9495aa82ca68bbec66e1c",
      "placeholder": "​",
      "style": "IPY_MODEL_142b1dfda9ae4444a1d577d1c66c215a",
      "value": "Downloading: 100%"
     }
    },
    "f4047dc7f8fe40e3858efacfe3fa9db3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e25e5700521472e83e6888067336682",
      "placeholder": "​",
      "style": "IPY_MODEL_ab24a9d6b7a84981ac9d335ff571f7e5",
      "value": "Generating train split:  99%"
     }
    },
    "f86f33b86afb468c8758174771d676c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9c217ef31284d95814342521904b45f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb7cce9fecc441b1baf720d2dea6dbfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb95bf53f6ab4ee19b1efeb17c2ea749": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a551f75352ac45b3845c61bbd5c3bb3c",
       "IPY_MODEL_1a8214cc9451415483ce785b997032e6",
       "IPY_MODEL_e131d608e26f44f89e1b155fa8042df9"
      ],
      "layout": "IPY_MODEL_97c7cc3aa1cb438facac717b44861c71"
     }
    },
    "fcb0f73a2e314a8e935f0802f2f12f31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb7cce9fecc441b1baf720d2dea6dbfc",
      "placeholder": "​",
      "style": "IPY_MODEL_7086129269a04514aa49001d7e712216",
      "value": "Downloading: 100%"
     }
    },
    "fedae0a67e9a4f249d9d2ab527d973e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_280ab82e1c5d4dd8abc4bd15e58034af",
       "IPY_MODEL_8c59fe3638c344748d6eed8206e8975d",
       "IPY_MODEL_5f72f99cd4094dc292146e35a4d67767"
      ],
      "layout": "IPY_MODEL_35e94aa9d357405780025da79e3691af"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
