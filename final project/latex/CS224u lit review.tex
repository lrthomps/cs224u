% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage[hang,flushmargin]{footmisc}

\title{CS224u lit review}

\author{Lara Thompson \\
  Principle Data Scientist @ Salesforce \\
  \texttt{lara.thompson@salesforce.com} \\ \\
  \today
}

\begin{document}

\maketitle

% \section*{Notes}

% This is a short paper ($\approx$6 page) summarizing and synthesizing several papers in the area of your final project. As noted above, 8 pages is the maximum allowed length.

% Groups of one should review 5 papers, groups of two should review 7 papers, and groups of three should review 9.

% The ideal is to have the same topic for your lit review and final project, but it's possible that you'll discover in the lit review that your topic isn't ideal for you, so you can switch topics (or groups) for the final project; your lit review will be graded on its own terms. 

% The following section headings are not required, but they nicely cover the main things you need to include.

% You are required to have a references section. The entries should appear alphabetically and give at least full author name(s), year of publication, title, and outlet if applicable (e.g., journal name or proceedings name). Beyond that, we are not picky about the format. Electronic references are fine but need to include the above information in addition to the link.

\section{General problem/task definition}

For my final project, I want to explore using large language models in a recommender system for deployment at scale with limited resources. As mentioned in the course, recommender systems are similar to retrieval systems albeit with a different aim: rather than searching for a few key results to answer a query, a recommender system has no query and should sometimes be less reliant on a current context (even if some information is given, e.g. last book read, we still don't know what the reader is in the mood for next). Regardless, there is much to learn from information retriever systems such as ColBERT \cite{ColBERT}, particularly for its introduction of late interactions. 

Sentence-BERT \cite{Reimers2019} is a good paper to compare with ColBERT in their usage of pre-trained BERT and arguably is a better starting place for ColBERT's fine-tuning that BERT itself. "A Simple but Tough-to-Beat Baseline for Sentence Embeddings" \cite{Arora2017} is a very interesting word embeddings aggregation approach that may suffice, especially if system performance is more important than the last few points of accuracy. 

Rec-BERT fine-tunes BERT to improve item-to-item text content-based recommendations \cite{Malkiel2020}; at that time, recommenders with text content that also considered user/item attributes and interactions used custom text embeddings that must be trained separately for each application / domain.

More recently, researchers from Alibaba showcase M6-Rec \cite{Cui2022} for all their downstream tasks (recommendations, chat, personal product design) by converting these tasks to text prompts. Although, this may a preview of where recommenders are headed, as we'll see, the LLMs may not be ready in their current state. 

\section{Concise summaries of the articles} 

\subsection{A Simple but Tough-to-Beat Baseline for Sentence Embeddings \cite{Arora2017}}

In contrast to Sentence-BERT, Arora et. al. from Princeton developed a very simple sentence embedding motivated from corpus generation theory. They take a weighted average of the word embeddings (downweighting common words) then subtract out the projection to the principal component (as estimated across several sentences; to remove another common discourse component).

The motivation for their approach is a modified random walk model of corpus generation. Rather than generating only words near a slowly changing discourse vector $c_s(t)$, they allow for the large deviations to common words ('the', 'and', etc.) in two ways: first, by separating a common discourse component $c_0$ and, second, by allowing any word to appear out of context in proportion to their typical frequency $p(w)$. Note that $c_s$, $c_0$ and $v_w$, the word embedding of $w$ all reside in the same embedding space. The original probability of observing a word $w$ is:
\begin{equation}
  \mathrm{Pr}[w(t)|c_s(t)] \propto \exp\left(\left< c_s(t), v_w \right> \right)
\end{equation}
After the two modifications, this becomes:
\begin{equation}
  \mathrm{Pr}[w(t)|c_s(t)] = \alpha p(w) + (1-\alpha) {\exp\left(\left< \tilde c_s(t), v_w \right> \right) \over Z_{\tilde c_s}}
\end{equation}
where $\tilde c_s = \beta c_0 + (1-\beta)c_s$. The authors go on to show how this relates to a $p(w)$-weighted embedding average that then has its component along the principle component $\sim c_0$.

They test their sentence embeddings across the STS benchmarks and compare against other sentence embedding approaches (not Sentence BERT since it came out later!). They find that they beat many more complicated approaches in text similarity tasks but that the RNN approaches outperform them on sentiment tasks. It seems a bag-of-words approach cannot capture the combined sentiment; in fact, this approach may specifically downweight various negation terms (e.g. "not") because they are common.

\subsection{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks \cite{Reimers2019}}

Reimers and Gurevych develop sentence embeddings that improve over various word embeddings-based approaches for text similarity tasks, that, nevertheless, scaled far better than BERT / RoBERTa that must process every pair individually. 

They use pre-trained BERT and RoBERTa in two main configurations (usually SRoBERTA lagged SBERT and will be dropped from the summary): in a siamese network fine-tuned on SNLI \cite{Bowman2015}, Multi-Genre NLI \cite{Williams2018} and (separately) on STS Argument Facet Similarity \cite{Misra2016} and STS benchmarks \cite{Cer2017}; and in a triplet network using the Wikipedia sections distinction dataset \cite{Dor2018}. They test both BERT-base and BERT-large.

The SNLI and multi NLI datasets are familiar from the course; the AFS dataset similarly has labelled pairs with a 0-5 similarity rating but, being excerpts from dialogue/arguments, the notion of similarity extends to the line reasoning of the argument. While SBERT-NLI\footnote[1]{Notation: SBERT-DATASET denotes SBERT fine-tuned on DATASET.} typically outperforms direct application of BERT on STS benchmarks; on the AFS dataset, the excerpt-spanning attention in BERT appears to be important and SBERT-AFS lags by several points in this task. 

The Wikipedia sections distinction dataset involves triplets: with two passages from one section of a Wikipedia article and a third from another section. They train with a triplet network (the three passages go through a single BERT model) and use a triplet loss that pushes the embeddings for similar passages closer and the dissimilar passages further in embedding space. This must be a challenging dataset to learn from: the sentences within a given section are not always semantically related; but it's an ingenious way to generate a very large dataset. SBERT-WikiSec does quite well.

As a final evaluation, SBERT-NLI is tested in a transfer learning setting \cite{Conneau2018}: even though SBERT is intended to be fine-tuned to each task (here it is tested on held out tasks), it outperformed many other sentence embeddings in all but a few tasks, even using BERT-base. 

\subsection{ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT \cite{ColBERT}}

ColBERT sets out to leverage the full quality of BERT for contextualized information retrieval while reducing query time by two orders of magnitude. Their design lands in the happy middle ground for query-document interactions in the network: late enough that the bulk of the expensive computations can be precomputed and indexed and yet there is an interaction layer outperforms completely separate query/document embeddings. 

Like SBERT, the same BERT model is used to encode both query and document ([\texttt{Q}]/[\texttt{D}] markers are prepended to denote which a sequence is). The query is padded with the special [\texttt{mask}] token that effectively augment the query in a differentiable way (effectively, their embeddings are infilled while attending to the rest of the query). The output layer is fed into a linear layer to reduce the final embedding dimensionality. These embeddings are used in the late interaction by taking the sum of maximum similarity between each query token and set of document tokens. The BERT layers are fine-tuned while the linear layer and [\texttt{Q}]/[\texttt{D}] marker's embeddings are trained from scratch using a pairwise softmax cross-entropy loss on triples of $\left< q, d^+, d^-\right>$ (a query and positive + negative document match). 

The form of this late interaction allows for very efficient querying: from faster loading of embeddings into the GPU to a form amenable to optimized large-scale vector-similarity search (specifically, \texttt{faiss} \cite{faiss}).

ColBERT was evaluated on two information retrieval benchmark datasets: MS Marco Ranking \cite{MSMarco} and TREC Complex Retrieval \cite{TREC-CAR}. On MS Marco in "re-ranking" mode, the MRR@10 of ColBERT using BERT-base outperformed the far more costly direct adaptation of BERT-base to ranking \cite{Nogueira2019}. In "end-to-end" mode, Colbert with BERT-large approaches the performance of BERT-large re-ranker, despite being ~500x faster. 

In the ablation study, the biggest improvements in MRR@10 were for maximum similarity in the late interaction (versus taking the average query to document embeddings) and for adding query augmentation via [\texttt{mask}] token padding. 

% \subsection{Graph Neural Networks in Recommender Systems: A Survey \cite{GGN4R2022}}

% This survey paper from last year covers a wide breadth of design choices for GNNs used for recommenders. Their taxonomy of GNNs as recommenders include “user-item collaborative filtering, sequential recommendation, social recommendation, knowledge graph-based recommendation, and other tasks” [1]; they review the challenges and solutions for each category. They conclude with a few interesting avenues of future work. My main interests lie in a combination of the first two so I’ll focus on the main design choices in these applications.

% The two main challenges in user-item are effectiveness (how well can graph structure learn complex user-item relationships) and efficiency (how feasible is training the graph on the full neighbourhoods of each node). Any solution is a compromise between the two issues: adding more complexity to the neighbour aggregation slows down training. Typically, the simplest mean or sum pooling suffices, or an attention module can be added to learn which neighbours to attend to most.

% For sequential recommendations, the main issue is that most training sequences are too short. Most solutions involve adding more edges either from other related sequences or from other relations (eg. 2nd hop item relations from a user-item graph). Attention over earlier states in the sequence with respect to the last state is often used to improve prediction of the next item.

% Relevant future work concerns dynamic graphs (how can new nodes or edges be used to update the representations), the reception field (a fixed number of hops will include too wide of a reception field for highly linked nodes compared to relatively inactive or new nodes), and self supervised learning (such as in masked item / edge learning). 

\subsection{RecoBERT: A Catalog Language Model for Text-Based Recommendations \cite{Malkiel2020}}

RecoBERT is an early example of using a pre-trained LLM for text-based item recommendations, a pure item-to-item recommender system. Earlier attempts that also incorporated context and/or user-item interactions (e.g. \cite{Djuric2015}, \cite{Zheng2017} or \cite{deSouza2018}) trained text embeddings from scratch with custom networks, making that most likely more costly to develop and less transferable. 

Title, document pairs are input to BERT-large as the sequence
\begin{equation*}
  \text{[CLS][title tokens][SEP][document tokens]}
\end{equation*}
15\% of the tokens are masked. The output embeddings for the title and document are separately averaged to give title and document level embeddings. The title-description model (TDM) loss is:
\begin{align}
  \begin{split}
  \mathcal{L}_{TDM} = -{1 \over 2}\sum_{i=1}^n & \left(  y_i \log \left(C_{TDM}^i \right) \right. + \\
              &  \left. (1-y_i)\log \left( 1- C_{TDM}^i \right) \right)
  \end{split}
\end{align} 
where $C_{TDM}$ is the cosine distance between the title/document embeddings.
A mask language model (MLM) component (following \cite{Devlin2019}) includes a classification layer mapping the BERT [CLS] embedding back to vocabulary space; the total loss becomes
\begin{equation}
  \mathcal{L} = \mathcal{L}_{TDM} + \mathcal{L}_{MLM}
\end{equation}

For inference, to test the similarity of a candidate title$^\prime$, document$^\prime$ to a known title, document they compute: cosine similarity of titles, cosine similarity of documents and the TDM model cosine distance crossed title, document$^\prime$ and title$^\prime$, document pairs; the sum of which defines their similarity metric.

They evaluate RecoBERT on two datasets: a wine review catalogue \cite{kagglewine} (with an expert annotated test set\footnote[2]{See the author's Github page for this paper, \url{https://github.com/r-papso/recobert}}), and a fashion catalog (no further details provided, presumably proprietary). RecoBERT is trained separately for each. They compare against other sentence embeddings, pre-trained BERT without fine-tuning, and BERT fine-tuned to each domain (on the same reviews that RecoBERT trains on but without the TDM head). RecoBERT outperformed the other approaches, often by a large margin, as quantified by either mean reciprocal ratio (MMR) or hit ratio (HR) for various top-$k$. 


\subsection{M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems \cite{Cui2022}}

M6 is a multi-modal (text and image) multilingual (Chinese and English) model and is already in wide use in Alibaba \cite{M6} (of the same generation as GPT-3). M6-Rec uses a pre-trained M6 model and reformulates all many downstream tasks as prompts by representing user behavior data as natural language plain text as a preface to recommended content using the special input format of M6. The plausibility of that recommendation is quantified using the probability of the output tokens for the recommendation half of the input sequence.

They further optimize is several ways. Inspired by ColBERT, late interactions segment the user attributes (e.g. "Male") and actions ("Clicked X.") to be precomputed for the bulk of the M6 layers. They are concatenated for the final 3 layers (with additionally learned positioning embeddings); interactions are only modelled in those last layers. They propose \emph{option tuning} (a modified prompt tuning \cite{PromptTuning}); combined with \emph{adaptation} \cite{Adaptation}, they achieve better CTR prediction than a fully fine-tuned M6. They need less than a million samples to out-perform the baseline model, DIN \cite{DIN}, given >100x more samples. Their one-shot performance on Amazon movie\footnote[3]{Presumably \url{https://snap.stanford.edu/data/web-Movies.html}. They have an unrelated reference in their paper: an error?} and Amazon Cloth\footnote[4]{Not sure which dataset, since again an unrelated reference is given and many such datasets exist.} is only matched by DIN after 40-50k samples, and they match DINs maximum performance with 400-shot learning. 

To further downsize, they distill, prune and quantize M6 down to from 300M parameters to the 2M parameter M6-edge model. They use early-exiting to further minimize inference time. The option tuning and adaptation can be fine-tuned on the M6-edge model directly on customers phones. They report M6-edge performance on Alibaba specific tasks and report only slight degradation from a full M6-base (they do not compare with M6-Rec).

The example they give of a personalized product design shows gender bias inherent in LLMs: given the context that a user
\begin{quote}
  "clicked product of category flowers and plants named Stephanotis floribunda, potted plants, evergreen, absorbing formaldehyde"
\end{quote}
and 
\begin{quote}
  "clicked product of category seasonings named Jiangxi dry fermented soybeans, handmade, black soybeans, Jiujiang speciality"
\end{quote}
the model predicts that this example user is a middle-aged housewife and suggests 
\begin{quote}
  "clicked product of category clothing named dress, middle-age housewife, summer clothing, chiffon, mid-length dresses, short sleeves"
\end{quote}
Sadly, the authors do not note this gender bias and think this is a wonderful suggestion. A recent study explores how even a bias-mitigated LLM transfers bias into harmful task-specific behavior after fine-tuning \cite{Steed2022-upstream}.

\section{Compare and contrast} 

These papers all pertain to sentence/passage embeddings: the first gives a baseline that should be tested against; SBERT developed versatile sentence embeddings in a python library; ColBERT optimized passage embeddings for fast information retrieval; RecoBERT fine-tuned BERT for item-to-item text recommendations using intermediate title/document embeddings; M6-Rec prompts their LLM in a context/recommendation format so that they can extract the probabilities of the recommendation embedding. 

The SBERT paper didn't include "a simple but tough-to-beat baseline for sentence embeddings" \cite{Arora2017} in their comparisons. Strangely, the two papers state fairly different performance for the same embeddings on the same tasks (e.g. simple average GloVe embeddings \cite{GloVe} on the STS benchmarks); either the GloVe embeddings used were trained on different corpuses or they're reporting different summaries of results across the subtasks ("a simple..." gave mean metrics across STS tasks and used the \href{https://nlp.stanford.edu/projects/glove/}{glove.840B.300d} embeddings; the SBERT paper doesn't say). Since both state a 5-20 point improvement over unweighted GloVe embedding averages, it would have been very interesting to see how SBERT performs against "a simple" baseline.

Like ColBERT, RecoBERT must to fine-tuned to each domain; in both cases, much of the heavy computation can be precomputed and indexed, although the RecoBERT authors did not consider this. RecoBERT is trained with a cosine loss to discriminate positive vs negative pairs; while both SBERT and ColBERT trained using a triplet loss. 

At the opposite extreme, M6-Rec does not fine-tune their LLM at all, but "augments" the model and trains only those additional terms (<1\% of the parameters of the entire model; presumably they must be trained per task). Each downstream task encodes the user and item context in a text prompt.

Another way to have one model for all tasks involves developing user embeddings that include all their attributes and interactions across all services. Every subtask works with these rich user embeddings and encode their products in the same vector space; each recommender can be much lighter weight. As a further benefit, the user feature store need only be accessed by the model training user embeddings. 

Many companies are adopting this pattern according to \cite{YanPatterns}: TripAdvisor creates user embeddings by aggregating item embeddings; YouTube aggregates video embeddings then concatenates other user attributes (geography, demographics, the age of the video, etc.); and Spotify learns session-level user embeddings on activity within and across sessions. At StitchFix, their users' clothing preferences and size may change together in time. They record attributes as a function of time so that, for example, a liked shirt style is recalled with the size the user had on file at that time \cite{StitchFixClientTime}.

% Point out the similarities and differences of the papers. Do they agree with each other? Are results seemingly in conflict? If the papers address different subtasks, how are they related? (If they are not related, then you may have made poor choices for a lit review...). This section is probably the most valuable for the final project, as it can become the basis for a lit review section.

\section{Future work} 

The simplest sentence embeddings may be improved by starting with BERT in-context word embeddings (e.g. taken from the first layer or two as we did in our first assignment). Their performance on sentiment tasks would be a good test since they did relatively poorly at them. 

Many studies suggest that a contrastive loss with triplets performs better; triplet loss works with the embeddings directly rather than forcing them first through bottleneck layers \cite{FaceNet}. If, as in M6-Rec and RecoBERT, the bottleneck is required for downsizing the embeddings, contrastive loss may be fine; but it's worth comparing with triplet loss for any improvements.

Furthermore, the efficacy of triplet training (and presumably contrastive training) relies on finding "good" positive/negative pairs (distinguishable but barely so), though gradient clipping must ease learning from too-hard pairs; in image and speech domains, much attention is given to choosing better pairs \cite{triplepairs}. Adapting these techniques to NLP may improve some of these approaches. 

The M6-Rec approach of one model for every subtask is a direction many companies are taking their recommender systems. From their paper, it doesn't seem wise to use a bias-ridden "wisdom of the crowd" for potentially rather specialized domains. To validating the wine recommender, the RecoBERT authors collected expert recommendations for a collection of wines; even though a user-item recommender trained within the wine community may not have that expertise either, it may still outperform the internet. 

For better scaling of recommendations, user embeddings seem like the best approach for now. Next is to add more modes of interaction (text first, then image, sound and video) and to formulate in time by learning from sequences of sessions.

% Make several suggestions for how the work can be extended. Are there open questions to answer? This would presumably include how the papers relate to your final project idea.



\bibliography{project}

% \appendix

% \section{Example Appendix}\label{sec:appendix}

% This is an appendix.

\end{document}
